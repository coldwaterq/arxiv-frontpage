{"created":"2024-05-28 17:59:53","title":"Notes on Applicability of GPT-4 to Document Understanding","abstract":"We perform a missing, reproducible evaluation of all publicly available GPT-4 family models concerning the Document Understanding field, where it is frequently required to comprehend text spacial arrangement and visual clues in addition to textual semantics. Benchmark results indicate that though it is hard to achieve satisfactory results with text-only models, GPT-4 Vision Turbo performs well when one provides both text recognized by an external OCR engine and document images on the input. Evaluation is followed by analyses that suggest possible contamination of textual GPT-4 models and indicate the significant performance drop for lengthy documents.","sentences":["We perform a missing, reproducible evaluation of all publicly available GPT-4 family models concerning the Document Understanding field, where it is frequently required to comprehend text spacial arrangement and visual clues in addition to textual semantics.","Benchmark results indicate that though it is hard to achieve satisfactory results with text-only models, GPT-4 Vision Turbo performs well when one provides both text recognized by an external OCR engine and document images on the input.","Evaluation is followed by analyses that suggest possible contamination of textual GPT-4 models and indicate the significant performance drop for lengthy documents."],"url":"http://arxiv.org/abs/2405.18433v1"}
{"created":"2024-05-28 17:59:51","title":"On the Origin of Llamas: Model Tree Heritage Recovery","abstract":"The rapid growth of neural network models shared on the internet has made model weights an important data modality. However, this information is underutilized as the weights are uninterpretable, and publicly available models are disorganized. Inspired by Darwin's tree of life, we define the Model Tree which describes the origin of models i.e., the parent model that was used to fine-tune the target model. Similarly to the natural world, the tree structure is unknown. In this paper, we introduce the task of Model Tree Heritage Recovery (MoTHer Recovery) for discovering Model Trees in the ever-growing universe of neural networks. Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights. Beyond the immediate application of model authorship attribution, MoTHer recovery holds exciting long-term applications akin to indexing the internet by search engines. Practically, for each pair of models, this task requires: i) determining if they are related, and ii) establishing the direction of the relationship. We find that certain distributional properties of the weights evolve monotonically during training, which enables us to classify the relationship between two given models. MoTHer recovery reconstructs entire model hierarchies, represented by a directed tree, where a parent model gives rise to multiple child models through additional training. Our approach successfully reconstructs complex Model Trees, as well as the structure of \"in-the-wild\" model families such as Llama 2 and Stable Diffusion.","sentences":["The rapid growth of neural network models shared on the internet has made model weights an important data modality.","However, this information is underutilized as the weights are uninterpretable, and publicly available models are disorganized.","Inspired by Darwin's tree of life, we define the Model Tree which describes the origin of models i.e., the parent model that was used to fine-tune the target model.","Similarly to the natural world, the tree structure is unknown.","In this paper, we introduce the task of Model Tree Heritage Recovery (MoTHer Recovery) for discovering Model Trees in the ever-growing universe of neural networks.","Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights.","Beyond the immediate application of model authorship attribution, MoTHer recovery holds exciting long-term applications akin to indexing the internet by search engines.","Practically, for each pair of models, this task requires: i) determining if they are related, and ii) establishing the direction of the relationship.","We find that certain distributional properties of the weights evolve monotonically during training, which enables us to classify the relationship between two given models.","MoTHer recovery reconstructs entire model hierarchies, represented by a directed tree, where a parent model gives rise to multiple child models through additional training.","Our approach successfully reconstructs complex Model Trees, as well as the structure of \"in-the-wild\" model families such as Llama 2 and Stable Diffusion."],"url":"http://arxiv.org/abs/2405.18432v1"}
{"created":"2024-05-28 17:59:42","title":"Feasibility of Privacy-Preserving Entity Resolution on Confidential Healthcare Datasets Using Homomorphic Encryption","abstract":"Patient datasets contain confidential information which is protected by laws and regulations such as HIPAA and GDPR. Ensuring comprehensive patient information necessitates privacy-preserving entity resolution (PPER), which identifies identical patient entities across multiple databases from different healthcare organizations while maintaining data privacy. Existing methods often lack cryptographic security or are computationally impractical for real-world datasets. We introduce a PPER pipeline based on AMPPERE, a secure abstract computation model utilizing cryptographic tools like homomorphic encryption. Our tailored approach incorporates extensive parallelization techniques and optimal parameters specifically for patient datasets. Experimental results demonstrate the proposed method's effectiveness in terms of accuracy and efficiency compared to various baselines.","sentences":["Patient datasets contain confidential information which is protected by laws and regulations such as HIPAA and GDPR.","Ensuring comprehensive patient information necessitates privacy-preserving entity resolution (PPER), which identifies identical patient entities across multiple databases from different healthcare organizations while maintaining data privacy.","Existing methods often lack cryptographic security or are computationally impractical for real-world datasets.","We introduce a PPER pipeline based on AMPPERE, a secure abstract computation model utilizing cryptographic tools like homomorphic encryption.","Our tailored approach incorporates extensive parallelization techniques and optimal parameters specifically for patient datasets.","Experimental results demonstrate the proposed method's effectiveness in terms of accuracy and efficiency compared to various baselines."],"url":"http://arxiv.org/abs/2405.18430v1"}
{"created":"2024-05-28 17:59:33","title":"DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention","abstract":"Diffusion models with large-scale pre-training have achieved significant success in the field of visual content generation, particularly exemplified by Diffusion Transformers (DiT). However, DiT models have faced challenges with scalability and quadratic complexity efficiency. In this paper, we aim to leverage the long sequence modeling capability of Gated Linear Attention (GLA) Transformers, expanding its applicability to diffusion models. We introduce Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable solution with minimal parameter overhead, following the DiT design, but offering superior efficiency and effectiveness. In addition to better performance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than DiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$. Moreover, we analyze the scalability of DiG across a variety of computational complexity. DiG models, with increased depth/width or augmentation of input tokens, consistently exhibit decreasing FID. We further compare DiG with other subquadratic-time diffusion models. With the same model size, DiG-XL/2 is $4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$ resolution, and is $1.8\\times$ faster than DiT with CUDA-optimized FlashAttention-2 under the $2048$ resolution. All these results demonstrate its superior efficiency among the latest diffusion models. Code is released at https://github.com/hustvl/DiG.","sentences":["Diffusion models with large-scale pre-training have achieved significant success in the field of visual content generation, particularly exemplified by Diffusion Transformers (DiT).","However, DiT models have faced challenges with scalability and quadratic complexity efficiency.","In this paper, we aim to leverage the long sequence modeling capability of Gated Linear Attention (GLA) Transformers, expanding its applicability to diffusion models.","We introduce Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable solution with minimal parameter overhead, following the DiT design, but offering superior efficiency and effectiveness.","In addition to better performance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than DiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.","Moreover, we analyze the scalability of DiG across a variety of computational complexity.","DiG models, with increased depth/width or augmentation of input tokens, consistently exhibit decreasing FID.","We further compare DiG with other subquadratic-time diffusion models.","With the same model size, DiG-XL/2 is $4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$ resolution, and is $1.8\\times$ faster than DiT with CUDA-optimized FlashAttention-2 under the $2048$ resolution.","All these results demonstrate its superior efficiency among the latest diffusion models.","Code is released at https://github.com/hustvl/DiG."],"url":"http://arxiv.org/abs/2405.18428v1"}
{"created":"2024-05-28 17:59:22","title":"GFlow: Recovering 4D World from Monocular Video","abstract":"Reconstructing 4D scenes from video inputs is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios. In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses. To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time. GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames. Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content. Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way. Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose. By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power. Visit our project website at: https://littlepure2333.github.io/GFlow","sentences":["Reconstructing 4D scenes from video inputs is a crucial yet challenging task.","Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios.","In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses.","To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time.","GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames.","Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content.","Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way.","Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose.","By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power.","Visit our project website at: https://littlepure2333.github.io/GFlow"],"url":"http://arxiv.org/abs/2405.18426v1"}
{"created":"2024-05-28 17:59:21","title":"ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention","abstract":"Recently, linear complexity sequence modeling networks have achieved modeling capabilities similar to Vision Transformers on a variety of computer vision tasks, while using fewer FLOPs and less memory. However, their advantage in terms of actual runtime speed is not significant. To address this issue, we introduce Gated Linear Attention (GLA) for vision, leveraging its superior hardware-awareness and efficiency. We propose direction-wise gating to capture 1D global context through bidirectional modeling and a 2D gating locality injection to adaptively inject 2D local details into 1D global context. Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency. The proposed model, \\name{}, offers a favorable trade-off in accuracy, parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy while using only 27\\% of the parameters and 20\\% of the FLOPs, running 2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution, \\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results position \\name{} as an efficient and scalable solution for visual representation learning. Code is available at \\url{https://github.com/hustvl/ViG}.","sentences":["Recently, linear complexity sequence modeling networks have achieved modeling capabilities similar to Vision Transformers on a variety of computer vision tasks, while using fewer FLOPs and less memory.","However, their advantage in terms of actual runtime speed is not significant.","To address this issue, we introduce Gated Linear Attention (GLA) for vision, leveraging its superior hardware-awareness and efficiency.","We propose direction-wise gating to capture 1D global context through bidirectional modeling and a 2D gating locality injection to adaptively inject 2D local details into 1D global context.","Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency.","The proposed model, \\name{}, offers a favorable trade-off in accuracy, parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer and CNN-based models.","Notably, \\name{}-S matches DeiT-B's accuracy while using only 27\\% of the parameters and 20\\% of the FLOPs, running 2$\\times$ faster on $224\\times224$ images.","At $1024\\times1024$ resolution, \\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results position \\name{} as an efficient and scalable solution for visual representation learning.","Code is available at \\url{https://github.com/hustvl/ViG}."],"url":"http://arxiv.org/abs/2405.18425v1"}
{"created":"2024-05-28 17:59:01","title":"3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting","abstract":"Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing. Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/.","sentences":["Scene image editing is crucial for entertainment, photography, and advertising design.","Existing methods solely focus on either 2D individual object or 3D global scene editing.","This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity.","In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects.","We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques.","Language features from CLIP then introduce semantics into 3D geometry for object disentanglement.","With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects.","Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing.","Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/."],"url":"http://arxiv.org/abs/2405.18424v1"}
{"created":"2024-05-28 17:57:23","title":"Hierarchical World Models as Visual Whole-Body Humanoid Controllers","abstract":"Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans. Code and videos: https://nicklashansen.com/rlpuppeteer","sentences":["Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology.","Learning from visual observations further exacerbates this difficulty.","In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives.","Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards.","Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.","Code and videos: https://nicklashansen.com/rlpuppeteer"],"url":"http://arxiv.org/abs/2405.18418v1"}
{"created":"2024-05-28 17:57:12","title":"3D StreetUnveiler with Semantic-Aware 2DGS","abstract":"Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporary static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scenes involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications. Project page and more visualizations can be found at: https://streetunveiler.github.io","sentences":["Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving.","However, removing all temporary static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge.","Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scenes involve long trajectories that differ from previous 3D inpainting tasks.","The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation.","To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street.","StreetUnveiler learns a 3D representation of the empty street from crowded observations.","Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed.","We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS.","Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map.","This decomposition helps us to minimize the regions that need to be inpainted.","To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations.","Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street.","The mesh representation of the empty street can be extracted for further applications.","Project page and more visualizations can be found at: https://streetunveiler.github.io"],"url":"http://arxiv.org/abs/2405.18416v1"}
{"created":"2024-05-28 17:57:06","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","abstract":"Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.","sentences":["Image classification is one of the most fundamental capabilities of machine vision intelligence.","In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA.","We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet.","To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs.","Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data.","Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models.","Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset."],"url":"http://arxiv.org/abs/2405.18415v1"}
{"created":"2024-05-28 17:56:46","title":"Don't Forget to Connect! Improving RAG with Graph-based Reranking","abstract":"Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models.","sentences":["Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents.","These systems work well when documents are clearly relevant to a question context.","But what about when a document has partial information, or less obvious connections to the context?","And how should we reason about connections between documents?","In this work, we seek to answer these two core questions about RAG generation.","We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG.","Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG.","G-RAG outperforms state-of-the-art approaches while having smaller computational footprint.","Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG.","This result emphasizes the importance of reranking for RAG even when using Large Language Models."],"url":"http://arxiv.org/abs/2405.18414v1"}
{"created":"2024-05-28 17:47:19","title":"Phased Consistency Model","abstract":"The consistency model (CM) has recently made significant progress in accelerating the generation of diffusion models. However, its application to high-resolution, text-conditioned image generation in the latent space (a.k.a., LCM) remains unsatisfactory. In this paper, we identify three key flaws in the current design of LCM. We investigate the reasons behind these limitations and propose the Phased Consistency Model (PCM), which generalizes the design space and addresses all identified limitations. Our evaluations demonstrate that PCM significantly outperforms LCM across 1--16 step generation settings. While PCM is specifically designed for multi-step refinement, it achieves even superior or comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show that PCM's methodology is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. More details are available at https://g-u-n.github.io/projects/pcm/.","sentences":["The consistency model (CM) has recently made significant progress in accelerating the generation of diffusion models.","However, its application to high-resolution, text-conditioned image generation in the latent space (a.k.a., LCM) remains unsatisfactory.","In this paper, we identify three key flaws in the current design of LCM.","We investigate the reasons behind these limitations and propose the Phased Consistency Model (PCM), which generalizes the design space and addresses all identified limitations.","Our evaluations demonstrate that PCM significantly outperforms LCM across 1--16 step generation settings.","While PCM is specifically designed for multi-step refinement, it achieves even superior or comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods.","Furthermore, we show that PCM's methodology is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator.","More details are available at https://g-u-n.github.io/projects/pcm/."],"url":"http://arxiv.org/abs/2405.18407v1"}
{"created":"2024-05-28 17:46:36","title":"RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives","abstract":"Recent video generative models primarily rely on carefully written text prompts for specific tasks, like inpainting or style editing. They require labor-intensive textual descriptions for input videos, hindering their flexibility to adapt personal/raw videos to user specifications. This paper proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video generative framework that supports multiple video editing capabilities such as removal, addition, and modification, through a unified pipeline. RACCooN consists of two principal stages: Video-to-Paragraph (V2P) and Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video scenes in well-structured natural language, capturing both the holistic context and focused object details. Subsequently, in the P2V stage, users can optionally refine these descriptions to guide the video diffusion model, enabling various modifications to the input video, such as removing, changing subjects, and/or adding new objects. The proposed approach stands out from other methods through several significant contributions: (1) RACCooN suggests a multi-granular spatiotemporal pooling strategy to generate well-structured video descriptions, capturing both the broad context and object details without requiring complex human annotations, simplifying precise video content editing based on text for users. (2) Our video generative model incorporates auto-generated narratives or instructions to enhance the quality and accuracy of the generated content. It supports the addition of video objects, inpainting, and attribute modification within a unified framework, surpassing existing video editing and inpainting benchmarks. The proposed framework demonstrates impressive versatile capabilities in video-to-paragraph generation, video content editing, and can be incorporated into other SoTA video generative models for further enhancement.","sentences":["Recent video generative models primarily rely on carefully written text prompts for specific tasks, like inpainting or style editing.","They require labor-intensive textual descriptions for input videos, hindering their flexibility to adapt personal/raw videos to user specifications.","This paper proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video generative framework that supports multiple video editing capabilities such as removal, addition, and modification, through a unified pipeline.","RACCooN consists of two principal stages: Video-to-Paragraph (V2P) and Paragraph-to-Video (P2V).","In the V2P stage, we automatically describe video scenes in well-structured natural language, capturing both the holistic context and focused object details.","Subsequently, in the P2V stage, users can optionally refine these descriptions to guide the video diffusion model, enabling various modifications to the input video, such as removing, changing subjects, and/or adding new objects.","The proposed approach stands out from other methods through several significant contributions: (1) RACCooN suggests a multi-granular spatiotemporal pooling strategy to generate well-structured video descriptions, capturing both the broad context and object details without requiring complex human annotations, simplifying precise video content editing based on text for users.","(2) Our video generative model incorporates auto-generated narratives or instructions to enhance the quality and accuracy of the generated content.","It supports the addition of video objects, inpainting, and attribute modification within a unified framework, surpassing existing video editing and inpainting benchmarks.","The proposed framework demonstrates impressive versatile capabilities in video-to-paragraph generation, video content editing, and can be incorporated into other SoTA video generative models for further enhancement."],"url":"http://arxiv.org/abs/2405.18406v1"}
{"created":"2024-05-28 17:46:27","title":"WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization","abstract":"Language has been useful in extending the vision encoder to data from diverse distributions without empirical discovery in training domains. However, as the image description is mostly at coarse-grained level and ignores visual details, the resulted embeddings are still ineffective in overcoming complexity of domains at inference time. We present a self-supervision framework WIDIn, Wording Images for Domain-Invariant representation, to disentangle discriminative visual representation, by only leveraging data in a single domain and without any test prior. Specifically, for each image, we first estimate the language embedding with fine-grained alignment, which can be consequently used to adaptively identify and then remove domain-specific counterpart from the raw visual embedding. WIDIn can be applied to both pretrained vision-language models like CLIP, and separately trained uni-modal models like MoCo and BERT. Experimental studies on three domain generalization datasets demonstrate the effectiveness of our approach.","sentences":["Language has been useful in extending the vision encoder to data from diverse distributions without empirical discovery in training domains.","However, as the image description is mostly at coarse-grained level and ignores visual details, the resulted embeddings are still ineffective in overcoming complexity of domains at inference time.","We present a self-supervision framework WIDIn, Wording Images for Domain-Invariant representation, to disentangle discriminative visual representation, by only leveraging data in a single domain and without any test prior.","Specifically, for each image, we first estimate the language embedding with fine-grained alignment, which can be consequently used to adaptively identify and then remove domain-specific counterpart from the raw visual embedding.","WIDIn can be applied to both pretrained vision-language models like CLIP, and separately trained uni-modal models like MoCo and BERT.","Experimental studies on three domain generalization datasets demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.18405v1"}
{"created":"2024-05-28 17:43:16","title":"Explicit Formulae to Interchangeably use Hyperplanes and Hyperballs using Inversive Geometry","abstract":"Many algorithms require discriminative boundaries, such as separating hyperplanes or hyperballs, or are specifically designed to work on spherical data. By applying inversive geometry, we show that the two discriminative boundaries can be used interchangeably, and that general Euclidean data can be transformed into spherical data, whenever a change in point distances is acceptable. We provide explicit formulae to embed general Euclidean data into spherical data and to unembed it back. We further show a duality between hyperspherical caps, i.e., the volume created by a separating hyperplane on spherical data, and hyperballs and provide explicit formulae to map between the two. We further provide equations to translate inner products and Euclidean distances between the two spaces, to avoid explicit embedding and unembedding. We also provide a method to enforce projections of the general Euclidean space onto hemi-hyperspheres and propose an intrinsic dimensionality based method to obtain \"all-purpose\" parameters. To show the usefulness of the cap-ball-duality, we discuss example applications in machine learning and vector similarity search.","sentences":["Many algorithms require discriminative boundaries, such as separating hyperplanes or hyperballs, or are specifically designed to work on spherical data.","By applying inversive geometry, we show that the two discriminative boundaries can be used interchangeably, and that general Euclidean data can be transformed into spherical data, whenever a change in point distances is acceptable.","We provide explicit formulae to embed general Euclidean data into spherical data and to unembed it back.","We further show a duality between hyperspherical caps, i.e., the volume created by a separating hyperplane on spherical data, and hyperballs and provide explicit formulae to map between the two.","We further provide equations to translate inner products and Euclidean distances between the two spaces, to avoid explicit embedding and unembedding.","We also provide a method to enforce projections of the general Euclidean space onto hemi-hyperspheres and propose an intrinsic dimensionality based method to obtain \"all-purpose\" parameters.","To show the usefulness of the cap-ball-duality, we discuss example applications in machine learning and vector similarity search."],"url":"http://arxiv.org/abs/2405.18401v1"}
{"created":"2024-05-28 17:40:48","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass","abstract":"Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the $k$ most recent token embeddings from the drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.","sentences":["Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions.","Under the hood, language models support this by running an autoregressive inference pass to provide a draft.","Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times.","To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass.","We achieve this by feeding a superposition of the $k$ most recent token embeddings from the drafts as input to the next decoding step of the language model.","At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations.","Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling.","Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding."],"url":"http://arxiv.org/abs/2405.18400v1"}
{"created":"2024-05-28 17:35:05","title":"MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations","abstract":"A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (>10x speedup).","sentences":["A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints.","Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation.","The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure.","In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations).","Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram).","We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution.","Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation.","It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (>10x speedup)."],"url":"http://arxiv.org/abs/2405.18395v1"}
{"created":"2024-05-28 17:33:57","title":"A Critique of Snapshot Isolation","abstract":"The support for transactions is an essential part of a database management system (DBMS). Without this support, the developers are burdened with ensuring atomic execution of a transaction despite failures as well as concurrent accesses to the database by other transactions. Ideally, a transactional system provides serializability, which means that the outcome of concurrent transactions is equivalent to a serial execution of them. Based on experiences on lock-based implementations, nevertheless, serializability is known as an expensive feature that comes with high overhead and low concurrency. Commercial systems, hence, compromise serializability by implementing weaker guarantees such as snapshot isolation. The developers, therefore, are still burdened with the anomalies that could arise due to the lack of serializability.   There have been recent attempts to enrich large-scale data stores, such as HBase and BigTable, with transactional support. Not surprisingly, inspired by traditional database management systems, serializability is usually compromised for the benefit of efficiency. For example, Google Percolator, implements lock-based snapshot isolation on top of BigTable. We show in this paper that this compromise is not necessary in lock-free implementations of transactional support. We introduce write-snapshot isolation, a novel isolation level that has a performance comparable with that of snapshot isolation, and yet provides serializability. The main insight in write-snapshot isolation is to prevent read-write conflicts in contrast to write-write conflicts that are prevented by snapshot isolation.","sentences":["The support for transactions is an essential part of a database management system (DBMS).","Without this support, the developers are burdened with ensuring atomic execution of a transaction despite failures as well as concurrent accesses to the database by other transactions.","Ideally, a transactional system provides serializability, which means that the outcome of concurrent transactions is equivalent to a serial execution of them.","Based on experiences on lock-based implementations, nevertheless, serializability is known as an expensive feature that comes with high overhead and low concurrency.","Commercial systems, hence, compromise serializability by implementing weaker guarantees such as snapshot isolation.","The developers, therefore, are still burdened with the anomalies that could arise due to the lack of serializability.   ","There have been recent attempts to enrich large-scale data stores, such as HBase and BigTable, with transactional support.","Not surprisingly, inspired by traditional database management systems, serializability is usually compromised for the benefit of efficiency.","For example, Google Percolator, implements lock-based snapshot isolation on top of BigTable.","We show in this paper that this compromise is not necessary in lock-free implementations of transactional support.","We introduce write-snapshot isolation, a novel isolation level that has a performance comparable with that of snapshot isolation, and yet provides serializability.","The main insight in write-snapshot isolation is to prevent read-write conflicts in contrast to write-write conflicts that are prevented by snapshot isolation."],"url":"http://arxiv.org/abs/2405.18393v1"}
{"created":"2024-05-28 17:33:54","title":"Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations","abstract":"Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative - constant learning rate and cooldowns - and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs.","sentences":["Scale has become a main ingredient in obtaining strong machine learning models.","As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures.","In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size.","We investigate the training behavior of a direct alternative - constant learning rate and cooldowns - and find that it scales predictably and reliably similar to cosine.","Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales.","Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs."],"url":"http://arxiv.org/abs/2405.18392v1"}
{"created":"2024-05-28 17:28:20","title":"Natural numbers from integers","abstract":"In homotopy type theory, a natural number type is freely generated by an element and an endomorphism. Similarly, an integer type is freely generated by an element and an automorphism. Using only dependent sums, identity types, extensional dependent products, and a type of two elements with large elimination, we construct a natural number type from an integer type. As a corollary, homotopy type theory with only $\\Sigma$, $\\mathsf{Id}$, $\\Pi$, and finite colimits with descent (and no universes) admits a natural number type. This improves and simplifies a result by Rose.","sentences":["In homotopy type theory, a natural number type is freely generated by an element and an endomorphism.","Similarly, an integer type is freely generated by an element and an automorphism.","Using only dependent sums, identity types, extensional dependent products, and a type of two elements with large elimination, we construct a natural number type from an integer type.","As a corollary, homotopy type theory with only $\\Sigma$, $\\mathsf{Id}$, $\\Pi$, and finite colimits with descent (and no universes) admits a natural number type.","This improves and simplifies a result by Rose."],"url":"http://arxiv.org/abs/2405.18388v1"}
{"created":"2024-05-28 17:27:24","title":"A Review and Implementation of Object Detection Models and Optimizations for Real-time Medical Mask Detection during the COVID-19 Pandemic","abstract":"Convolutional Neural Networks (CNN) are commonly used for the problem of object detection thanks to their increased accuracy. Nevertheless, the performance of CNN-based detection models is ambiguous when detection speed is considered. To the best of our knowledge, there has not been sufficient evaluation of the available methods in terms of the speed/accuracy trade-off in related literature. This work assesses the most fundamental object detection models on the Common Objects in Context (COCO) dataset with respect to this trade-off, their memory consumption, and computational and storage cost. Next, we select a highly efficient model called YOLOv5 to train on the topical and unexplored dataset of human faces with medical masks, the Properly-Wearing Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization techniques for real-time medical mask detection: transfer learning, data augmentations, and a Squeeze-and-Excitation attention mechanism. Using our findings in the context of the COVID-19 pandemic, we propose an optimized model based on YOLOv5s using transfer learning for the detection of correctly and incorrectly worn medical masks that surpassed more than two times in speed (69 frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset while maintaining the same level of mean Average Precision (67%).","sentences":["Convolutional Neural Networks (CNN) are commonly used for the problem of object detection thanks to their increased accuracy.","Nevertheless, the performance of CNN-based detection models is ambiguous when detection speed is considered.","To the best of our knowledge, there has not been sufficient evaluation of the available methods in terms of the speed/accuracy trade-off in related literature.","This work assesses the most fundamental object detection models on the Common Objects in Context (COCO) dataset with respect to this trade-off, their memory consumption, and computational and storage cost.","Next, we select a highly efficient model called YOLOv5 to train on the topical and unexplored dataset of human faces with medical masks, the Properly-Wearing Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization techniques for real-time medical mask detection: transfer learning, data augmentations, and a Squeeze-and-Excitation attention mechanism.","Using our findings in the context of the COVID-19 pandemic, we propose an optimized model based on YOLOv5s using transfer learning for the detection of correctly and incorrectly worn medical masks that surpassed more than two times in speed (69 frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset while maintaining the same level of mean Average Precision (67%)."],"url":"http://arxiv.org/abs/2405.18387v1"}
{"created":"2024-05-28 17:27:20","title":"Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning","abstract":"Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.","sentences":["Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation.","Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction.","To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems.","Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music.","Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks.","This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments."],"url":"http://arxiv.org/abs/2405.18386v1"}
{"created":"2024-05-28 17:26:57","title":"Blocking Tracking JavaScript at the Function Granularity","abstract":"Modern websites extensively rely on JavaScript to implement both functionality and tracking. Existing privacy enhancing content blocking tools struggle against mixed scripts, which simultaneously implement both functionality and tracking, because blocking the script would break functionality and not blocking it would allow tracking. We propose Not.js, a fine grained JavaScript blocking tool that operates at the function level granularity. Not.js's strengths lie in analyzing the dynamic execution context, including the call stack and calling context of each JavaScript function, and then encoding this context to build a rich graph representation. Not.js trains a supervised machine learning classifier on a webpage's graph representation to first detect tracking at the JavaScript function level and then automatically generate surrogate scripts that preserve functionality while removing tracking. Our evaluation of Not.js on the top 10K websites demonstrates that it achieves high precision (94%) and recall (98%) in detecting tracking JavaScript functions, outperforming the state of the art while being robust against off the shelf JavaScript obfuscation. Fine grained detection of tracking functions allows Not.js to automatically generate surrogate scripts that remove tracking JavaScript functions without causing major breakage. Our deployment of Not.js shows that mixed scripts are present on 62.3% of the top 10K websites, with 70.6% of the mixed scripts being third party that engage in tracking activities such as cookie ghostwriting. We share a sample of the tracking functions detected by Not.js within mixed scripts not currently on filter lists with filter list authors, who confirm that these scripts are not blocked due to potential functionality breakage, despite being known to implement tracking.","sentences":["Modern websites extensively rely on JavaScript to implement both functionality and tracking.","Existing privacy enhancing content blocking tools struggle against mixed scripts, which simultaneously implement both functionality and tracking, because blocking the script would break functionality and not blocking it would allow tracking.","We propose Not.js, a fine grained JavaScript blocking tool that operates at the function level granularity.","Not.js's strengths lie in analyzing the dynamic execution context, including the call stack and calling context of each JavaScript function, and then encoding this context to build a rich graph representation.","Not.js trains a supervised machine learning classifier on a webpage's graph representation to first detect tracking at the JavaScript function level and then automatically generate surrogate scripts that preserve functionality while removing tracking.","Our evaluation of Not.js on the top 10K websites demonstrates that it achieves high precision (94%) and recall (98%) in detecting tracking JavaScript functions, outperforming the state of the art while being robust against off the shelf JavaScript obfuscation.","Fine grained detection of tracking functions allows Not.js to automatically generate surrogate scripts that remove tracking JavaScript functions without causing major breakage.","Our deployment of Not.js shows that mixed scripts are present on 62.3% of the top 10K websites, with 70.6% of the mixed scripts being third party that engage in tracking activities such as cookie ghostwriting.","We share a sample of the tracking functions detected by Not.js within mixed scripts not currently on filter lists with filter list authors, who confirm that these scripts are not blocked due to potential functionality breakage, despite being known to implement tracking."],"url":"http://arxiv.org/abs/2405.18385v1"}
{"created":"2024-05-28 17:25:43","title":"Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation","abstract":"The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT) challenge aims to advance automated segmentation algorithms using the largest known multi-institutional dataset of radiotherapy planning brain MRIs with expert-annotated target labels for patients with intact or post-operative meningioma that underwent either conventional external beam radiotherapy or stereotactic radiosurgery. Each case includes a defaced 3D post-contrast T1-weighted radiotherapy planning MRI in its native acquisition space, accompanied by a single-label \"target volume\" representing the gross tumor volume (GTV) and any at-risk post-operative site. Target volume annotations adhere to established radiotherapy planning protocols, ensuring consistency across cases and institutions. For pre-operative meningiomas, the target volume encompasses the entire GTV and associated nodular dural tail, while for post-operative cases, it includes at-risk resection cavity margins as determined by the treating institution. Case annotations were reviewed and approved by expert neuroradiologists and radiation oncologists. Participating teams will develop, containerize, and evaluate automated segmentation models using this comprehensive dataset. Model performance will be assessed using the lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The top-performing teams will be recognized at the Medical Image Computing and Computer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is expected to significantly advance automated radiotherapy planning by enabling precise tumor segmentation and facilitating tailored treatment, ultimately improving patient outcomes.","sentences":["The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT) challenge aims to advance automated segmentation algorithms using the largest known multi-institutional dataset of radiotherapy planning brain MRIs with expert-annotated target labels for patients with intact or post-operative meningioma that underwent either conventional external beam radiotherapy or stereotactic radiosurgery.","Each case includes a defaced 3D post-contrast T1-weighted radiotherapy planning MRI in its native acquisition space, accompanied by a single-label \"target volume\" representing the gross tumor volume (GTV) and any at-risk post-operative site.","Target volume annotations adhere to established radiotherapy planning protocols, ensuring consistency across cases and institutions.","For pre-operative meningiomas, the target volume encompasses the entire GTV and associated nodular dural tail, while for post-operative cases, it includes at-risk resection cavity margins as determined by the treating institution.","Case annotations were reviewed and approved by expert neuroradiologists and radiation oncologists.","Participating teams will develop, containerize, and evaluate automated segmentation models using this comprehensive dataset.","Model performance will be assessed using the lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance.","The top-performing teams will be recognized at the Medical Image Computing and Computer Assisted Intervention Conference in October 2024.","BraTS-MEN-RT is expected to significantly advance automated radiotherapy planning by enabling precise tumor segmentation and facilitating tailored treatment, ultimately improving patient outcomes."],"url":"http://arxiv.org/abs/2405.18383v1"}
{"created":"2024-05-28 17:22:22","title":"OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning","abstract":"The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors. We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained. Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs. To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner. By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning. Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory.","sentences":["The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks.","However, the substantial size of LLMs presents significant challenges in training or fine-tuning.","While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning.","In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors.","We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained.","Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs.","To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner.","By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning.","Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning.","Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient.","OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory."],"url":"http://arxiv.org/abs/2405.18380v1"}
{"created":"2024-05-28 17:22:15","title":"A Canonization Perspective on Invariant and Equivariant Learning","abstract":"In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonization perspective that provides an essential and complete view of the design of frames. Canonization is a classic approach for attaining invariance by mapping inputs to their canonical forms. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods -- some are even optimal -- both theoretically and empirically. The reduction to the canonization perspective further uncovers equivalences between previous methods. These observations suggest that canonization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods.","sentences":["In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data.","Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames.","What we currently lack is a principled understanding of the design of frames.","In this work, we introduce a canonization perspective that provides an essential and complete view of the design of frames.","Canonization is a classic approach for attaining invariance by mapping inputs to their canonical forms.","We show that there exists an inherent connection between frames and canonical forms.","Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames.","Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods -- some are even optimal -- both theoretically and empirically.","The reduction to the canonization perspective further uncovers equivalences between previous methods.","These observations suggest that canonization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods."],"url":"http://arxiv.org/abs/2405.18378v1"}
{"created":"2024-05-28 17:20:44","title":"LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models","abstract":"The abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption. Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms. To mitigate this, we propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures. We show that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex. More specifically, we demonstrate a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy. In addition to finding smaller, higher-performing network architectures, our method does so more effectively and efficiently than certain pruning or sparsification techniques. Finally, we demonstrate how quantization is complementary to our method and that the size and complexity of the networks we find can be further decreased using quantization. We believe that our work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms.","sentences":["The abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption.","Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms.","To mitigate this, we propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS.","In particular, we fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures.","We show that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex.","More specifically, we demonstrate a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy.","In addition to finding smaller, higher-performing network architectures, our method does so more effectively and efficiently than certain pruning or sparsification techniques.","Finally, we demonstrate how quantization is complementary to our method and that the size and complexity of the networks we find can be further decreased using quantization.","We believe that our work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms."],"url":"http://arxiv.org/abs/2405.18377v1"}
{"created":"2024-05-28 17:18:17","title":"Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning","abstract":"Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target domain using only unlabeled target data. Current SFDA methods face challenges in effectively leveraging pre-trained knowledge and exploiting target domain data. Multimodal Large Language Models (MLLMs) offer remarkable capabilities in understanding visual and textual information, but their applicability to SFDA poses challenges such as instruction-following failures, intensive computational demands, and difficulties in performance measurement prior to adaptation. To alleviate these issues, we propose Reliability-based Curriculum Learning (RCL), a novel framework that integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in SFDA. Our framework incorporates proposed Reliable Knowledge Transfer, Self-correcting and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to progressively exploit unlabeled data in the target domain. RCL achieves state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g., $\\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing adaptability and robustness without requiring access to source data. Code: https://github.com/Dong-Jie-Chen/RCL.","sentences":["Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target domain using only unlabeled target data.","Current SFDA methods face challenges in effectively leveraging pre-trained knowledge and exploiting target domain data.","Multimodal Large Language Models (MLLMs) offer remarkable capabilities in understanding visual and textual information, but their applicability to SFDA poses challenges such as instruction-following failures, intensive computational demands, and difficulties in performance measurement prior to adaptation.","To alleviate these issues, we propose Reliability-based Curriculum Learning (RCL), a novel framework that integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in SFDA.","Our framework incorporates proposed Reliable Knowledge Transfer, Self-correcting and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to progressively exploit unlabeled data in the target domain.","RCL achieves state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g., $\\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing adaptability and robustness without requiring access to source data.","Code: https://github.com/Dong-Jie-Chen/RCL."],"url":"http://arxiv.org/abs/2405.18376v1"}
{"created":"2024-05-28 17:14:02","title":"Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning","abstract":"Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it. However, only a few of these benchmarks are available in languages other than English. Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages. This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.   Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges. We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art. Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning.","sentences":["Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it.","However, only a few of these benchmarks are available in languages other than English.","Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages.","This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.   ","Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges.","We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art.","Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning."],"url":"http://arxiv.org/abs/2405.18375v1"}
{"created":"2024-05-28 17:12:41","title":"Hostile Counterspeech Drives Users From Hate Subreddits","abstract":"Counterspeech -- speech that opposes hate speech -- has gained significant attention recently as a strategy to reduce hate on social media. While previous studies suggest that counterspeech can somewhat reduce hate speech, little is known about its effects on participation in online hate communities, nor which counterspeech tactics reduce harmful behavior. We begin to address these gaps by identifying 25 large hate communities (\"subreddits\") within Reddit and analyzing the effect of counterspeech on newcomers within these communities. We first construct a new public dataset of carefully annotated counterspeech and non-counterspeech comments within these subreddits. We use this dataset to train a state-of-the-art counterspeech detection model. Next, we use matching to evaluate the causal effects of hostile and non-hostile counterspeech on the engagement of newcomers in hate subreddits. We find that, while non-hostile counterspeech is ineffective at keeping users from fully disengaging from these hate subreddits, a single hostile counterspeech comment substantially reduces both future likelihood of engagement. While offering nuance to the understanding of counterspeech efficacy, these results a) leave unanswered the question of whether hostile counterspeech dissuades newcomers from participation in online hate writ large, or merely drives them into less-moderated and more extreme hate communities, and b) raises ethical considerations about hostile counterspeech, which is both comparatively common and might exacerbate rather than mitigate the net level of antagonism in society. These findings underscore the importance of future work to improve counterspeech tactics and minimize unintended harm.","sentences":["Counterspeech -- speech that opposes hate speech -- has gained significant attention recently as a strategy to reduce hate on social media.","While previous studies suggest that counterspeech can somewhat reduce hate speech, little is known about its effects on participation in online hate communities, nor which counterspeech tactics reduce harmful behavior.","We begin to address these gaps by identifying 25 large hate communities (\"subreddits\") within Reddit and analyzing the effect of counterspeech on newcomers within these communities.","We first construct a new public dataset of carefully annotated counterspeech and non-counterspeech comments within these subreddits.","We use this dataset to train a state-of-the-art counterspeech detection model.","Next, we use matching to evaluate the causal effects of hostile and non-hostile counterspeech on the engagement of newcomers in hate subreddits.","We find that, while non-hostile counterspeech is ineffective at keeping users from fully disengaging from these hate subreddits, a single hostile counterspeech comment substantially reduces both future likelihood of engagement.","While offering nuance to the understanding of counterspeech efficacy, these results a) leave unanswered the question of whether hostile counterspeech dissuades newcomers from participation in online hate writ large, or merely drives them into less-moderated and more extreme hate communities, and b) raises ethical considerations about hostile counterspeech, which is both comparatively common and might exacerbate rather than mitigate the net level of antagonism in society.","These findings underscore the importance of future work to improve counterspeech tactics and minimize unintended harm."],"url":"http://arxiv.org/abs/2405.18374v1"}
{"created":"2024-05-28 17:08:31","title":"PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework","abstract":"Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities. Central to their success is the concept of prompting, which guides model output generation. However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions. This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks. Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance. The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity. It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance. PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization.","sentences":["Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities.","Central to their success is the concept of prompting, which guides model output generation.","However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions.","This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks.","Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance.","The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity.","It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance.","PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs.","Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization."],"url":"http://arxiv.org/abs/2405.18369v1"}
{"created":"2024-05-28 17:07:55","title":"The 2024 Brain Tumor Segmentation (BraTS) Challenge: Glioma Segmentation on Post-treatment MRI","abstract":"Gliomas are the most common malignant primary brain tumors in adults and one of the deadliest types of cancer. There are many challenges in treatment and monitoring due to the genetic diversity and high intrinsic heterogeneity in appearance, shape, histology, and treatment response. Treatments include surgery, radiation, and systemic therapies, with magnetic resonance imaging (MRI) playing a key role in treatment planning and post-treatment longitudinal assessment. The 2024 Brain Tumor Segmentation (BraTS) challenge on post-treatment glioma MRI will provide a community standard and benchmark for state-of-the-art automated segmentation models based on the largest expert-annotated post-treatment glioma MRI dataset. Challenge competitors will develop automated segmentation models to predict four distinct tumor sub-regions consisting of enhancing tissue (ET), surrounding non-enhancing T2/fluid-attenuated inversion recovery (FLAIR) hyperintensity (SNFH), non-enhancing tumor core (NETC), and resection cavity (RC). Models will be evaluated on separate validation and test datasets using standardized performance metrics utilized across the BraTS 2024 cluster of challenges, including lesion-wise Dice Similarity Coefficient and Hausdorff Distance. Models developed during this challenge will advance the field of automated MRI segmentation and contribute to their integration into clinical practice, ultimately enhancing patient care.","sentences":["Gliomas are the most common malignant primary brain tumors in adults and one of the deadliest types of cancer.","There are many challenges in treatment and monitoring due to the genetic diversity and high intrinsic heterogeneity in appearance, shape, histology, and treatment response.","Treatments include surgery, radiation, and systemic therapies, with magnetic resonance imaging (MRI) playing a key role in treatment planning and post-treatment longitudinal assessment.","The 2024 Brain Tumor Segmentation (BraTS) challenge on post-treatment glioma MRI will provide a community standard and benchmark for state-of-the-art automated segmentation models based on the largest expert-annotated post-treatment glioma MRI dataset.","Challenge competitors will develop automated segmentation models to predict four distinct tumor sub-regions consisting of enhancing tissue (ET), surrounding non-enhancing T2/fluid-attenuated inversion recovery (FLAIR) hyperintensity (SNFH), non-enhancing tumor core (NETC), and resection cavity (RC).","Models will be evaluated on separate validation and test datasets using standardized performance metrics utilized across the BraTS 2024 cluster of challenges, including lesion-wise Dice Similarity Coefficient and Hausdorff Distance.","Models developed during this challenge will advance the field of automated MRI segmentation and contribute to their integration into clinical practice, ultimately enhancing patient care."],"url":"http://arxiv.org/abs/2405.18368v1"}
{"created":"2024-05-28 17:05:29","title":"Black Hole Search in Dynamic Graphs","abstract":"A black hole in a graph is a dangerous site that disposes any incoming agent into that node without leaving any trace of its existence. In the Black Hole Search (BHS) problem, the goal is for at least one agent to survive, locate the position of the black hole, and then terminate. This problem has been extensively studied for static graphs, where the edges do not disappear with time. In dynamic graphs, where the edges may disappear and reappear with time, the problem has only been studied for specific graphs such as rings and cactuses. In this work, we investigate the problem of BHS for general graphs with a much weaker model with respect to the one used for the cases of rings and cactus graphs\\cite{bhattacharya_2023, Paola_2024}. We consider two cases: (a) where the adversary can remove at most one edge in each round, and (b) where the adversary can remove at most $f$ edges in each round. In both scenarios, we consider rooted configuration.   In the case when the adversary can remove at most one edge from the graph, we provide an algorithm that uses 9 agents to solve the BHS problem in $O(m^2)$ time given that each node $v$ is equipped with $O(\\log \\delta_v)$ storage in the form of a whiteboard, where $m$ is the number of edges in $G$ and $\\delta_v$ is the degree of node $v$. We also prove that it is impossible for $2\\delta_{BH}$ many agents with $O(\\log n)$ memory to locate the black hole where $\\delta_{BH}$ is the degree of the black hole even if the nodes are equipped with whiteboards of $O(\\log \\delta_v)$ storage.   In a scenario where the adversary can remove at most $f$ edges and the initial configuration is rooted, we present an algorithm that uses $6f$ agents to solve the BHS problem. We also prove that solving BHS using $2f+1$ agents starting from a rooted configuration on a general graph is impossible, even with unlimited node storage and infinite agent memory.","sentences":["A black hole in a graph is a dangerous site that disposes any incoming agent into that node without leaving any trace of its existence.","In the Black Hole Search (BHS) problem, the goal is for at least one agent to survive, locate the position of the black hole, and then terminate.","This problem has been extensively studied for static graphs, where the edges do not disappear with time.","In dynamic graphs, where the edges may disappear and reappear with time, the problem has only been studied for specific graphs such as rings and cactuses.","In this work, we investigate the problem of BHS for general graphs with a much weaker model with respect to the one used for the cases of rings and cactus graphs\\cite{bhattacharya_2023, Paola_2024}.","We consider two cases: (a) where the adversary can remove at most one edge in each round, and (b) where the adversary can remove at most $f$ edges in each round.","In both scenarios, we consider rooted configuration.   ","In the case when the adversary can remove at most one edge from the graph, we provide an algorithm that uses 9 agents to solve the BHS problem in $O(m^2)$ time given that each node $v$ is equipped with $O(\\log \\delta_v)$ storage in the form of a whiteboard, where $m$ is the number of edges in $G$ and $\\delta_v$ is the degree of node $v$. We also prove that it is impossible for $2\\delta_{BH}$ many agents with $O(\\log n)$ memory to locate the black hole where $\\delta_{BH}$ is the degree of the black hole even if the nodes are equipped with whiteboards of $O(\\log \\delta_v)$ storage.   ","In a scenario where the adversary can remove at most $f$ edges and the initial configuration is rooted, we present an algorithm that uses $6f$ agents to solve the BHS problem.","We also prove that solving BHS using $2f+1$ agents starting from a rooted configuration on a general graph is impossible, even with unlimited node storage and infinite agent memory."],"url":"http://arxiv.org/abs/2405.18367v1"}
{"created":"2024-05-28 16:57:44","title":"Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?","abstract":"Rapid advancements in Autonomous Driving (AD) tasks turned a significant shift toward end-to-end fashion, particularly in the utilization of vision-language models (VLMs) that integrate robust logical reasoning and cognitive abilities to enable comprehensive end-to-end planning. However, these VLM-based approaches tend to integrate 2D vision tokenizers and a large language model (LLM) for ego-car planning, which lack 3D geometric priors as a cornerstone of reliable planning. Naturally, this observation raises a critical concern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Our evaluation of current VLM-based methods across 3D object detection, vectorized map construction, and environmental caption suggests that the answer is, unfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliable autonomous driving. In response, we introduce DETR-style 3D perceptrons as 3D tokenizers, which connect LLM with a one-layer linear projector. This simple yet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D physical world, enabling it to simultaneously process high-resolution multi-view images and employ spatiotemporal modeling. Despite its simplicity, Atlas demonstrates superior performance in both 3D detection and ego planning tasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable autonomous driving. The code and datasets will be released.","sentences":["Rapid advancements in Autonomous Driving (AD) tasks turned a significant shift toward end-to-end fashion, particularly in the utilization of vision-language models (VLMs) that integrate robust logical reasoning and cognitive abilities to enable comprehensive end-to-end planning.","However, these VLM-based approaches tend to integrate 2D vision tokenizers and a large language model (LLM) for ego-car planning, which lack 3D geometric priors as a cornerstone of reliable planning.","Naturally, this observation raises a critical concern: Can a 2D-tokenized LLM accurately perceive the 3D environment?","Our evaluation of current VLM-based methods across 3D object detection, vectorized map construction, and environmental caption suggests that the answer is, unfortunately, NO.","In other words, 2D-tokenized LLM fails to provide reliable autonomous driving.","In response, we introduce DETR-style 3D perceptrons as 3D tokenizers, which connect LLM with a one-layer linear projector.","This simple yet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D physical world, enabling it to simultaneously process high-resolution multi-view images and employ spatiotemporal modeling.","Despite its simplicity, Atlas demonstrates superior performance in both 3D detection and ego planning tasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable autonomous driving.","The code and datasets will be released."],"url":"http://arxiv.org/abs/2405.18361v1"}
{"created":"2024-05-28 16:56:42","title":"Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs","abstract":"Large language models (LLMs) are at the forefront of transforming numerous domains globally. However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency. First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages. Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance. Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time. This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies. Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages.","sentences":["Large language models (LLMs) are at the forefront of transforming numerous domains globally.","However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages.","This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning.","Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape.","Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency.","First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages.","Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance.","Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time.","This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies.","Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages."],"url":"http://arxiv.org/abs/2405.18359v1"}
{"created":"2024-05-28 16:55:41","title":"MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning","abstract":"Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language. However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information. This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks. Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning. Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities. Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines.","sentences":["Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language.","However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information.","This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks.","Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning.","Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities.","Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines."],"url":"http://arxiv.org/abs/2405.18358v1"}
{"created":"2024-05-28 16:55:33","title":"Faithful Logical Reasoning via Symbolic Chain-of-Thought","abstract":"While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.","sentences":["While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules.","To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting.","Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain.","Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances.","We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning.","To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs.","Code is open at https://github.com/Aiden0526/SymbCoT."],"url":"http://arxiv.org/abs/2405.18357v1"}
{"created":"2024-05-28 16:52:52","title":"Simulating infinite-dimensional nonlinear diffusion bridges","abstract":"The diffusion bridge is a type of diffusion process that conditions on hitting a specific state within a finite time period. It has broad applications in fields such as Bayesian inference, financial mathematics, control theory, and shape analysis. However, simulating the diffusion bridge for natural data can be challenging due to both the intractability of the drift term and continuous representations of the data. Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain unresolved. In the paper, we present a solution to this problem by merging score-matching techniques with operator learning, enabling a direct approach to score-matching for the infinite-dimensional bridge. We construct the score to be discretization invariant, which is natural given the underlying spatially continuous process. We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data, and our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training.","sentences":["The diffusion bridge is a type of diffusion process that conditions on hitting a specific state within a finite time period.","It has broad applications in fields such as Bayesian inference, financial mathematics, control theory, and shape analysis.","However, simulating the diffusion bridge for natural data can be challenging due to both the intractability of the drift term and continuous representations of the data.","Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain unresolved.","In the paper, we present a solution to this problem by merging score-matching techniques with operator learning, enabling a direct approach to score-matching for the infinite-dimensional bridge.","We construct the score to be discretization invariant, which is natural given the underlying spatially continuous process.","We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data, and our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training."],"url":"http://arxiv.org/abs/2405.18353v1"}
{"created":"2024-05-28 16:50:42","title":"Evolutionary Algorithms for Optimizing Emergency Exit Placement in Indoor Environments","abstract":"The problem of finding the optimal placement of emergency exits in an indoor environment to facilitate the rapid and orderly evacuation of crowds is addressed in this work. A cellular-automaton model is used to simulate the behavior of pedestrians in such scenarios, taking into account factors such as the environment, the pedestrians themselves, and the interactions among them. A metric is proposed to determine how successful or satisfactory an evacuation was. Subsequently, two metaheuristic algorithms, namely an iterated greedy heuristic and an evolutionary algorithm (EA) are proposed to solve the optimization problem. A comparative analysis shows that the proposed EA is able to find effective solutions for different scenarios, and that an island-based version of it outperforms the other two algorithms in terms of solution quality.","sentences":["The problem of finding the optimal placement of emergency exits in an indoor environment to facilitate the rapid and orderly evacuation of crowds is addressed in this work.","A cellular-automaton model is used to simulate the behavior of pedestrians in such scenarios, taking into account factors such as the environment, the pedestrians themselves, and the interactions among them.","A metric is proposed to determine how successful or satisfactory an evacuation was.","Subsequently, two metaheuristic algorithms, namely an iterated greedy heuristic and an evolutionary algorithm (EA) are proposed to solve the optimization problem.","A comparative analysis shows that the proposed EA is able to find effective solutions for different scenarios, and that an island-based version of it outperforms the other two algorithms in terms of solution quality."],"url":"http://arxiv.org/abs/2405.18352v1"}
{"created":"2024-05-28 16:49:28","title":"Evaluating Bayesian deep learning for radio galaxy classification","abstract":"The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories. Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs. In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem.","sentences":["The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories.","Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs.","In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem."],"url":"http://arxiv.org/abs/2405.18351v1"}
{"created":"2024-05-28 16:48:05","title":"A System for Automatic English Text Expansion","abstract":"We present an automatic text expansion system to generate English sentences, which performs automatic Natural Language Generation (NLG) by combining linguistic rules with statistical approaches. Here, \"automatic\" means that the system can generate coherent and correct sentences from a minimum set of words. From its inception, the design is modular and adaptable to other languages. This adaptability is one of its greatest advantages. For English, we have created the highly precise aLexiE lexicon with wide coverage, which represents a contribution on its own. We have evaluated the resulting NLG library in an Augmentative and Alternative Communication (AAC) proof of concept, both directly (by regenerating corpus sentences) and manually (from annotations) using a popular corpus in the NLG field. We performed a second analysis by comparing the quality of text expansion in English to Spanish, using an ad-hoc Spanish-English parallel corpus. The system might also be applied to other domains such as report and news generation.","sentences":["We present an automatic text expansion system to generate English sentences, which performs automatic Natural Language Generation (NLG) by combining linguistic rules with statistical approaches.","Here, \"automatic\" means that the system can generate coherent and correct sentences from a minimum set of words.","From its inception, the design is modular and adaptable to other languages.","This adaptability is one of its greatest advantages.","For English, we have created the highly precise aLexiE lexicon with wide coverage, which represents a contribution on its own.","We have evaluated the resulting NLG library in an Augmentative and Alternative Communication (AAC) proof of concept, both directly (by regenerating corpus sentences) and manually (from annotations) using a popular corpus in the NLG field.","We performed a second analysis by comparing the quality of text expansion in English to Spanish, using an ad-hoc Spanish-English parallel corpus.","The system might also be applied to other domains such as report and news generation."],"url":"http://arxiv.org/abs/2405.18350v1"}
{"created":"2024-05-28 16:44:02","title":"Can Automatic Metrics Assess High-Quality Translations?","abstract":"Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods.","sentences":["Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments.","However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source.","In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality.","This effect is most pronounced when the quality is high and the variance among alternatives is low.","Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality.","Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans.","Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods."],"url":"http://arxiv.org/abs/2405.18348v1"}
{"created":"2024-05-28 16:43:57","title":"Dataset Growth","abstract":"Deep learning benefits from the growing abundance of available data. Meanwhile, efficiently dealing with the growing data scale has become a challenge. Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale. There are existing techniques for cleaning/selecting the collected data. However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems. In practice, data are growing exponentially with both problems. This leads to repeated data curation with sub-optimal efficiency. To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity. InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design. Its framework makes it practical for real-world data engines.","sentences":["Deep learning benefits from the growing abundance of available data.","Meanwhile, efficiently dealing with the growing data scale has become a challenge.","Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale.","There are existing techniques for cleaning/selecting the collected data.","However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems.","In practice, data are growing exponentially with both problems.","This leads to repeated data curation with sub-optimal efficiency.","To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity.","InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design.","Its framework makes it practical for real-world data engines."],"url":"http://arxiv.org/abs/2405.18347v1"}
{"created":"2024-05-28 16:43:41","title":"Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation","abstract":"Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.","sentences":["Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety.","This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes.","We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs).","The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care.","Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings.","The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care."],"url":"http://arxiv.org/abs/2405.18346v1"}
{"created":"2024-05-28 16:42:43","title":"The Battle of LLMs: A Comparative Study in Conversational QA Tasks","abstract":"Large language models have gained considerable interest for their impressive performance on various tasks. Within this domain, ChatGPT and GPT-4, developed by OpenAI, and the Gemini, developed by Google, have emerged as particularly popular among early adopters. Additionally, Mixtral by Mistral AI and Claude by Anthropic are newly released, further expanding the landscape of advanced language models. These models are viewed as disruptive technologies with applications spanning customer service, education, healthcare, and finance. More recently, Mistral has entered the scene, captivating users with its unique ability to generate creative content. Understanding the perspectives of these users is crucial, as they can offer valuable insights into the potential strengths, weaknesses, and overall success or failure of these technologies in various domains. This research delves into the responses generated by ChatGPT, GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora. Evaluation scores were meticulously computed and subsequently compared to ascertain the overall performance of these models. Our study pinpointed instances where these models provided inaccurate answers to questions, offering insights into potential areas where they might be susceptible to errors. In essence, this research provides a comprehensive comparison and evaluation of these state of-the-art language models, shedding light on their capabilities while also highlighting potential areas for improvement","sentences":["Large language models have gained considerable interest for their impressive performance on various tasks.","Within this domain, ChatGPT and GPT-4, developed by OpenAI, and the Gemini, developed by Google, have emerged as particularly popular among early adopters.","Additionally, Mixtral by Mistral AI and Claude by Anthropic are newly released, further expanding the landscape of advanced language models.","These models are viewed as disruptive technologies with applications spanning customer service, education, healthcare, and finance.","More recently, Mistral has entered the scene, captivating users with its unique ability to generate creative content.","Understanding the perspectives of these users is crucial, as they can offer valuable insights into the potential strengths, weaknesses, and overall success or failure of these technologies in various domains.","This research delves into the responses generated by ChatGPT, GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.","Evaluation scores were meticulously computed and subsequently compared to ascertain the overall performance of these models.","Our study pinpointed instances where these models provided inaccurate answers to questions, offering insights into potential areas where they might be susceptible to errors.","In essence, this research provides a comprehensive comparison and evaluation of these state of-the-art language models, shedding light on their capabilities while also highlighting potential areas for improvement"],"url":"http://arxiv.org/abs/2405.18344v1"}
{"created":"2024-05-28 16:32:24","title":"What characteristics define disinformation and fake news?: review of taxonomies and definitions","abstract":"What characteristics define disinformation and fake news? To address this research question, this Technical Note provides a comprehensive analysis of disinformation and fake news, synthesizing 46 definitions and highlighting four key points addressing their fundamental characteristics. Adopting the Prisma 2020 method, five search sets with the Boolean operator AND were selected in both Portuguese and English, which were applied across four databases, resulting in 237 reviewed articles. Following a meticulous analysis, relevant articles were identified and included, while duplicates and inaccessible documents were excluded. It points to disinformation as information that is totally or partially false, crafted by a sender with the aim of misleading, with opportunistic content designed to manipulate reality, being amplified by individual characteristics of the receiver in their interpretation and by contextual factors in which they are embedded. This Technical Note seeks to contribute to an understanding of the phenomenon of disinformation that includes the contextual dimension, obtaining as fundamental elements of analysis: I.) Sender; II.) Content; III.) Receiver; and IV.) Environment.","sentences":["What characteristics define disinformation and fake news?","To address this research question, this Technical Note provides a comprehensive analysis of disinformation and fake news, synthesizing 46 definitions and highlighting four key points addressing their fundamental characteristics.","Adopting the Prisma 2020 method, five search sets with the Boolean operator AND were selected in both Portuguese and English, which were applied across four databases, resulting in 237 reviewed articles.","Following a meticulous analysis, relevant articles were identified and included, while duplicates and inaccessible documents were excluded.","It points to disinformation as information that is totally or partially false, crafted by a sender with the aim of misleading, with opportunistic content designed to manipulate reality, being amplified by individual characteristics of the receiver in their interpretation and by contextual factors in which they are embedded.","This Technical Note seeks to contribute to an understanding of the phenomenon of disinformation that includes the contextual dimension, obtaining as fundamental elements of analysis:","I.) Sender; II.)","Content; III.)","Receiver; and IV.)","Environment."],"url":"http://arxiv.org/abs/2405.18339v1"}
{"created":"2024-05-28 16:30:36","title":"Approximating Densest Subgraph in Geometric Intersection Graphs","abstract":"$ \\newcommand{\\cardin}[1]{\\left| {#1} \\right|}% \\newcommand{\\Graph}{\\Mh{\\mathsf{G}}}% \\providecommand{\\G}{\\Graph}% \\renewcommand{\\G}{\\Graph}% \\providecommand{\\GA}{\\Mh{H}}% \\renewcommand{\\GA}{\\Mh{H}}% \\newcommand{\\VV}{\\Mh{\\mathsf{V}}}% \\newcommand{\\VX}[1]{\\VV\\pth{#1}}% \\providecommand{\\EE}{\\Mh{\\mathsf{E}}}% \\renewcommand{\\EE}{\\Mh{\\mathsf{E}}}% \\newcommand{\\Re}{\\mathbb{R}} \\newcommand{\\reals}{\\mathbb{R}} \\newcommand{\\SetX}{\\mathsf{X}} \\newcommand{\\rad}{r} \\newcommand{\\Mh}[1]{#1} \\newcommand{\\query}{q} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\VorX}[1]{\\mathcal{V} \\pth{#1}} \\newcommand{\\Polygon}{\\mathsf{P}} \\newcommand{\\IntRange}[1]{[ #1 ]} \\newcommand{\\Space}{\\overline{\\mathsf{m}}} \\newcommand{\\pth}[2][\\!]{#1\\left({#2}\\right)} \\newcommand{\\polylog}{\\mathrm{polylog}} \\newcommand{\\N}{\\mathbb N} \\newcommand{\\Z}{\\mathbb Z} \\newcommand{\\pt}{p} \\newcommand{\\distY}[2]{\\left\\| {#1} - {#2} \\right\\|} \\newcommand{\\ptq}{q} \\newcommand{\\pts}{s}$ For an undirected graph $\\mathsf{G}=(\\mathsf{V}, \\mathsf{E})$, with $n$ vertices and $m$ edges, the \\emph{densest subgraph} problem, is to compute a subset $S \\subseteq \\mathsf{V}$ which maximizes the ratio $|\\mathsf{E}_S| / |S|$, where $\\mathsf{E}_S \\subseteq \\mathsf{E}$ is the set of all edges of $\\mathsf{G}$ with endpoints in $S$. The densest subgraph problem is a well studied problem in computer science. Existing exact and approximation algorithms for computing the densest subgraph require $\\Omega(m)$ time. We present near-linear time (in $n$) approximation algorithms for the densest subgraph problem on \\emph{implicit} geometric intersection graphs, where the vertices are explicitly given but not the edges. As a concrete example, we consider $n$ disks in the plane with arbitrary radii and present two different approximation algorithms.","sentences":["$ \\newcommand{\\cardin}[1]{\\left| {#1} \\right|}% \\newcommand{\\Graph}{\\Mh{\\mathsf{G}}}% \\providecommand{\\G}{\\Graph}% \\renewcommand{\\G}{\\Graph}% \\providecommand{\\GA}{\\Mh{H}}% \\renewcommand{\\GA}{\\Mh{H}}% \\newcommand{\\VV}{\\Mh{\\mathsf{V}}}% \\newcommand{\\VX}[1]{\\VV\\pth{#1}}% \\providecommand{\\EE}{\\Mh{\\mathsf{E}}}% \\renewcommand{\\EE}{\\Mh{\\mathsf{E}}}% \\newcommand{\\Re}{\\mathbb{R}} \\newcommand{\\reals}{\\mathbb{R}} \\newcommand{\\SetX}{\\mathsf{X}} \\newcommand{\\rad}{r} \\newcommand{\\Mh}[1]{#1} \\newcommand{\\query}{q} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\VorX}[1]{\\mathcal{V} \\pth{#1}} \\newcommand{\\Polygon}{\\mathsf{P}} \\newcommand{\\IntRange}[1]{[ #1 ]} \\newcommand{\\Space}{\\overline{\\mathsf{m}}} \\newcommand{\\pth}[2][\\!]{#1\\left({#2}\\right)} \\newcommand{\\polylog}{\\mathrm{polylog}} \\newcommand{\\N}{\\mathbb N} \\newcommand{\\Z}{\\mathbb Z} \\newcommand{\\pt}{p} \\newcommand{\\distY}[2]{\\left\\| {#1} - {#2} \\right\\|} \\newcommand{\\ptq}{q} \\newcommand{\\pts}{s}$","For an undirected graph $\\mathsf{G}=(\\mathsf{V}, \\mathsf{E})$, with $n$ vertices and $m$ edges, the \\emph{densest subgraph} problem, is to compute a subset $S \\subseteq \\mathsf{V}$ which maximizes the ratio $|\\mathsf{E}_S| / |S|$, where $\\mathsf{E}_S \\subseteq \\mathsf{E}$ is the set of all edges of $\\mathsf{G}$ with endpoints in $S$. The densest subgraph problem is a well studied problem in computer science.","Existing exact and approximation algorithms for computing the densest subgraph require $\\Omega(m)$ time.","We present near-linear time (in $n$) approximation algorithms for the densest subgraph problem on \\emph{implicit} geometric intersection graphs, where the vertices are explicitly given but not the edges.","As a concrete example, we consider $n$ disks in the plane with arbitrary radii and present two different approximation algorithms."],"url":"http://arxiv.org/abs/2405.18337v1"}
{"created":"2024-05-28 16:28:58","title":"Interpretable classification of wiki-review streams","abstract":"Wiki articles are created and maintained by a crowd of editors, producing a continuous stream of reviews. Reviews can take the form of additions, reverts, or both. This crowdsourcing model is exposed to manipulation since neither reviews nor editors are automatically screened and purged. To protect articles against vandalism or damage, the stream of reviews can be mined to classify reviews and profile editors in real-time. The goal of this work is to anticipate and explain which reviews to revert. This way, editors are informed why their edits will be reverted. The proposed method employs stream-based processing, updating the profiling and classification models on each incoming event. The profiling uses side and content-based features employing Natural Language Processing, and editor profiles are incrementally updated based on their reviews. Since the proposed method relies on self-explainable classification algorithms, it is possible to understand why a review has been classified as a revert or a non-revert. In addition, this work contributes an algorithm for generating synthetic data for class balancing, making the final classification fairer. The proposed online method was tested with a real data set from Wikivoyage, which was balanced through the aforementioned synthetic data generation. The results attained near-90 % values for all evaluation metrics (accuracy, precision, recall, and F-measure).","sentences":["Wiki articles are created and maintained by a crowd of editors, producing a continuous stream of reviews.","Reviews can take the form of additions, reverts, or both.","This crowdsourcing model is exposed to manipulation since neither reviews nor editors are automatically screened and purged.","To protect articles against vandalism or damage, the stream of reviews can be mined to classify reviews and profile editors in real-time.","The goal of this work is to anticipate and explain which reviews to revert.","This way, editors are informed why their edits will be reverted.","The proposed method employs stream-based processing, updating the profiling and classification models on each incoming event.","The profiling uses side and content-based features employing Natural Language Processing, and editor profiles are incrementally updated based on their reviews.","Since the proposed method relies on self-explainable classification algorithms, it is possible to understand why a review has been classified as a revert or a non-revert.","In addition, this work contributes an algorithm for generating synthetic data for class balancing, making the final classification fairer.","The proposed online method was tested with a real data set from Wikivoyage, which was balanced through the aforementioned synthetic data generation.","The results attained near-90 % values for all evaluation metrics (accuracy, precision, recall, and F-measure)."],"url":"http://arxiv.org/abs/2405.18335v1"}
{"created":"2024-05-28 16:28:51","title":"SketchQL Demonstration: Zero-shot Video Moment Querying with Sketches","abstract":"In this paper, we will present SketchQL, a video database management system (VDBMS) for retrieving video moments with a sketch-based query interface. This novel interface allows users to specify object trajectory events with simple mouse drag-and-drop operations. Users can use trajectories of single objects as building blocks to compose complex events. Using a pre-trained model that encodes trajectory similarity, SketchQL achieves zero-shot video moments retrieval by performing similarity searches over the video to identify clips that are the most similar to the visual query. In this demonstration, we introduce the graphic user interface of SketchQL and detail its functionalities and interaction mechanisms. We also demonstrate the end-to-end usage of SketchQL from query composition to video moments retrieval using real-world scenarios.","sentences":["In this paper, we will present SketchQL, a video database management system (VDBMS) for retrieving video moments with a sketch-based query interface.","This novel interface allows users to specify object trajectory events with simple mouse drag-and-drop operations.","Users can use trajectories of single objects as building blocks to compose complex events.","Using a pre-trained model that encodes trajectory similarity, SketchQL achieves zero-shot video moments retrieval by performing similarity searches over the video to identify clips that are the most similar to the visual query.","In this demonstration, we introduce the graphic user interface of SketchQL and detail its functionalities and interaction mechanisms.","We also demonstrate the end-to-end usage of SketchQL from query composition to video moments retrieval using real-world scenarios."],"url":"http://arxiv.org/abs/2405.18334v1"}
{"created":"2024-05-28 16:24:47","title":"Frustratingly Easy Test-Time Adaptation of Vision-Language Models","abstract":"Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples. For this reason, Episodic Test-Time Adaptation (TTA) strategies have recently emerged as powerful techniques to adapt VLMs in the presence of a single unlabeled image. The recent literature on TTA is dominated by the paradigm of prompt tuning by Marginal Entropy Minimization, which, relying on online backpropagation, inevitably slows down inference while increasing memory. In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it. We term this approach ZERO (TTA with \"zero\" temperature), whose design is both incredibly effective and frustratingly simple: augment N times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero. Remarkably, ZERO requires a single batched forward pass through the vision encoder only and no backward passes. We thoroughly evaluate our approach following the experimental protocol established in the literature and show that ZERO largely surpasses or compares favorably w.r.t. the state-of-the-art while being almost 10x faster and 13x more memory-friendly than standard Test-Time Prompt Tuning. Thanks to its simplicity and comparatively negligible computation, ZERO can serve as a strong baseline for future work in this field. The code is available at https://github.com/FarinaMatteo/zero.","sentences":["Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples.","For this reason, Episodic Test-Time Adaptation (TTA) strategies have recently emerged as powerful techniques to adapt VLMs in the presence of a single unlabeled image.","The recent literature on TTA is dominated by the paradigm of prompt tuning by Marginal Entropy Minimization, which, relying on online backpropagation, inevitably slows down inference while increasing memory.","In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it.","We term this approach ZERO (TTA with \"zero\" temperature), whose design is both incredibly effective and frustratingly simple: augment N times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero.","Remarkably, ZERO requires a single batched forward pass through the vision encoder only and no backward passes.","We thoroughly evaluate our approach following the experimental protocol established in the literature and show that ZERO largely surpasses or compares favorably w.r.t.","the state-of-the-art while being almost 10x faster and 13x more memory-friendly than standard Test-Time Prompt Tuning.","Thanks to its simplicity and comparatively negligible computation, ZERO can serve as a strong baseline for future work in this field.","The code is available at https://github.com/FarinaMatteo/zero."],"url":"http://arxiv.org/abs/2405.18330v1"}
{"created":"2024-05-28 16:22:18","title":"Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes","abstract":"Gaussian processes are a versatile probabilistic machine learning model whose effectiveness often depends on good hyperparameters, which are typically learned by maximising the marginal likelihood. In this work, we consider iterative methods, which use iterative linear system solvers to approximate marginal likelihood gradients up to a specified numerical precision, allowing a trade-off between compute time and accuracy of a solution. We introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, and identify that the computational costs are dominated by solving sequential batches of large positive-definite systems of linear equations. We then propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a $\\textit{warm start}$. Finally, we discuss the necessary conditions and quantify the consequences of warm starts and demonstrate their effectiveness on regression tasks, where warm starts achieve the same results as the conventional procedure while providing up to a $16 \\times$ average speed-up among datasets.","sentences":["Gaussian processes are a versatile probabilistic machine learning model whose effectiveness often depends on good hyperparameters, which are typically learned by maximising the marginal likelihood.","In this work, we consider iterative methods, which use iterative linear system solvers to approximate marginal likelihood gradients up to a specified numerical precision, allowing a trade-off between compute time and accuracy of a solution.","We introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, and identify that the computational costs are dominated by solving sequential batches of large positive-definite systems of linear equations.","We then propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a $\\textit{warm start}$.","Finally, we discuss the necessary conditions and quantify the consequences of warm starts and demonstrate their effectiveness on regression tasks, where warm starts achieve the same results as the conventional procedure while providing up to a $16 \\times$ average speed-up among datasets."],"url":"http://arxiv.org/abs/2405.18328v1"}
{"created":"2024-05-28 16:21:03","title":"VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers","abstract":"Video try-on stands as a promising area for its tremendous real-world potential. Prior works are limited to transferring product clothing images onto person videos with simple poses and backgrounds, while underperforming on casually captured videos. Recently, Sora revealed the scalability of Diffusion Transformer (DiT) in generating lifelike videos featuring real-world scenarios. Inspired by this, we explore and propose the first DiT-based video try-on framework for practical in-the-wild applications, named VITON-DiT. Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal denoising DiT, and an identity preservation ControlNet. To faithfully recover the clothing details, the extracted garment features are fused with the self-attention outputs of the denoising DiT and the ControlNet. We also introduce novel random selection strategies during training and an Interpolated Auto-Regressive (IAR) technique at inference to facilitate long video generation. Unlike existing attempts that require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability, VITON-DiT alleviates this by relying solely on unpaired human dance videos and a carefully designed multi-stage training strategy. Furthermore, we curate a challenging benchmark dataset to evaluate the performance of casual video try-on. Extensive experiments demonstrate the superiority of VITON-DiT in generating spatio-temporal consistent try-on results for in-the-wild videos with complicated human poses.","sentences":["Video try-on stands as a promising area for its tremendous real-world potential.","Prior works are limited to transferring product clothing images onto person videos with simple poses and backgrounds, while underperforming on casually captured videos.","Recently, Sora revealed the scalability of Diffusion Transformer (DiT) in generating lifelike videos featuring real-world scenarios.","Inspired by this, we explore and propose the first DiT-based video try-on framework for practical in-the-wild applications, named VITON-DiT. Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal denoising DiT, and an identity preservation ControlNet.","To faithfully recover the clothing details, the extracted garment features are fused with the self-attention outputs of the denoising DiT and the ControlNet.","We also introduce novel random selection strategies during training and an Interpolated Auto-Regressive (IAR) technique at inference to facilitate long video generation.","Unlike existing attempts that require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability, VITON-DiT alleviates this by relying solely on unpaired human dance videos and a carefully designed multi-stage training strategy.","Furthermore, we curate a challenging benchmark dataset to evaluate the performance of casual video try-on.","Extensive experiments demonstrate the superiority of VITON-DiT in generating spatio-temporal consistent try-on results for in-the-wild videos with complicated human poses."],"url":"http://arxiv.org/abs/2405.18326v1"}
{"created":"2024-05-28 16:20:33","title":"Value Alignment and Trust in Human-Robot Interaction: Insights from Simulation and User Study","abstract":"With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks. To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention. Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team. This assumption, however, has not been empirically verified. Moreover, prior literature does not account for human's trust in the robot when analyzing human-robot value alignment. Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust? Is it always beneficial to align the robot's values with that of the human? We present a simulation study and a human-subject study to answer these questions. Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high. We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction. Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values. We also present results from an empirical study that validate these findings from simulation. Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human's values.","sentences":["With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks.","To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention.","Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team.","This assumption, however, has not been empirically verified.","Moreover, prior literature does not account for human's trust in the robot when analyzing human-robot value alignment.","Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust?","Is it always beneficial to align the robot's values with that of the human?","We present a simulation study and a human-subject study to answer these questions.","Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high.","We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction.","Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values.","We also present results from an empirical study that validate these findings from simulation.","Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human's values."],"url":"http://arxiv.org/abs/2405.18324v1"}
{"created":"2024-05-28 16:14:10","title":"SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation","abstract":"Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data. To tackle this task, existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms, which neglect the dense prediction nature of the task, (2) aggregate them into memory-intensive hypercolumn formations, and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features. In this paper, we introduce SCE-MAE, a framework that (1) leverages the MAE, a region-level SSL method that naturally better suits the landmark prediction task, (2) operates on the vanilla feature map instead of on expensive hypercolumns, and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences. We demonstrate through extensive experiments that SCE-MAE is highly effective and robust, outperforming existing SOTA methods by large margins of approximately 20%-44% on the landmark matching and approximately 9%-15% on the landmark detection tasks.","sentences":["Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data.","To tackle this task, existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms, which neglect the dense prediction nature of the task, (2) aggregate them into memory-intensive hypercolumn formations, and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features.","In this paper, we introduce SCE-MAE, a framework that (1) leverages the MAE, a region-level SSL method that naturally better suits the landmark prediction task, (2) operates on the vanilla feature map instead of on expensive hypercolumns, and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences.","We demonstrate through extensive experiments that SCE-MAE is highly effective and robust, outperforming existing SOTA methods by large margins of approximately 20%-44% on the landmark matching and approximately 9%-15% on the landmark detection tasks."],"url":"http://arxiv.org/abs/2405.18322v1"}
{"created":"2024-05-28 16:11:11","title":"Self-Supervised Learning Based Handwriting Verification","abstract":"We present SSL-HV: Self-Supervised Learning approaches applied to the task of Handwriting Verification. This task involves determining whether a given pair of handwritten images originate from the same or different writer distribution. We have compared the performance of multiple generative, contrastive SSL approaches against handcrafted feature extractors and supervised learning on CEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE) outperforms other generative approaches achieving 76.3% accuracy, while ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization (VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using a pre-trained VAE and VICReg for the downstream task of writer verification we observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18 supervised baseline with 10% writer labels.","sentences":["We present SSL-HV: Self-Supervised Learning approaches applied to the task of Handwriting Verification.","This task involves determining whether a given pair of handwritten images originate from the same or different writer distribution.","We have compared the performance of multiple generative, contrastive SSL approaches against handcrafted feature extractors and supervised learning on CEDAR AND dataset.","We show that ResNet based Variational Auto-Encoder (VAE) outperforms other generative approaches achieving 76.3% accuracy, while ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization (VICReg) outperforms other contrastive approaches achieving 78% accuracy.","Using a pre-trained VAE and VICReg for the downstream task of writer verification we observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18 supervised baseline with 10% writer labels."],"url":"http://arxiv.org/abs/2405.18320v1"}
{"created":"2024-05-28 16:07:45","title":"DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data","abstract":"In the era of artificial intelligence, the diversity of data modalities and annotation formats often renders data unusable directly, requiring understanding and format conversion before it can be used by researchers or developers with different needs. To tackle this problem, this article introduces a framework called Dataset Description Language (DSDL) that aims to simplify dataset processing by providing a unified standard for AI datasets. DSDL adheres to the three basic practical principles of generic, portable, and extensible, using a unified standard to express data of different modalities and structures, facilitating the dissemination of AI data, and easily extending to new modalities and tasks. The standardized specifications of DSDL reduce the workload for users in data dissemination, processing, and usage. To further improve user convenience, we provide predefined DSDL templates for various tasks, convert mainstream datasets to comply with DSDL specifications, and provide comprehensive documentation and DSDL tools. These efforts aim to simplify the use of AI data, thereby improving the efficiency of AI development.","sentences":["In the era of artificial intelligence, the diversity of data modalities and annotation formats often renders data unusable directly, requiring understanding and format conversion before it can be used by researchers or developers with different needs.","To tackle this problem, this article introduces a framework called Dataset Description Language (DSDL) that aims to simplify dataset processing by providing a unified standard for AI datasets.","DSDL adheres to the three basic practical principles of generic, portable, and extensible, using a unified standard to express data of different modalities and structures, facilitating the dissemination of AI data, and easily extending to new modalities and tasks.","The standardized specifications of DSDL reduce the workload for users in data dissemination, processing, and usage.","To further improve user convenience, we provide predefined DSDL templates for various tasks, convert mainstream datasets to comply with DSDL specifications, and provide comprehensive documentation and DSDL tools.","These efforts aim to simplify the use of AI data, thereby improving the efficiency of AI development."],"url":"http://arxiv.org/abs/2405.18315v1"}
{"created":"2024-05-28 16:07:17","title":"Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm","abstract":"Targeted and uniform interventions to a system are crucial for unveiling causal relationships. While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging. Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available. In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution. More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders. Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings. To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score. Intersort outperforms baselines (GIES, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field. Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions.","sentences":["Targeted and uniform interventions to a system are crucial for unveiling causal relationships.","While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging.","Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available.","In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution.","More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders.","Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings.","To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score.","Intersort outperforms baselines (GIES, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field.","Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions."],"url":"http://arxiv.org/abs/2405.18314v1"}
{"created":"2024-05-28 16:02:11","title":"Deterministic and statistical calibration of constitutive models from full-field data with parametric physics-informed neural networks","abstract":"The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities. In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest. However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches. Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated. In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation. In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration. We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data. We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty. A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid. Finally, we consider experimental data and show that the results are in good agreement with a Finite Element Method-based calibration. Due to the fast evaluation of PINNs, calibration can be performed in near real-time. This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference.","sentences":["The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities.","In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest.","However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches.","Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated.","In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation.","In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration.","We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data.","We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty.","A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid.","Finally, we consider experimental data and show that the results are in good agreement with a Finite Element Method-based calibration.","Due to the fast evaluation of PINNs, calibration can be performed in near real-time.","This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference."],"url":"http://arxiv.org/abs/2405.18311v1"}
{"created":"2024-05-28 16:01:19","title":"Joint Lemmatization and Morphological Tagging with LEMMING","abstract":"We present LEMMING, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. LEMMING sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial.","sentences":["We present LEMMING, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features.","It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers.","LEMMING sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58.","We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial."],"url":"http://arxiv.org/abs/2405.18308v1"}
{"created":"2024-05-28 15:58:31","title":"Multi-modal Generation via Cross-Modal In-Context Learning","abstract":"In this work, we study the problem of generating novel images from complex multimodal prompt sequences. While existing methods achieve promising results for text-to-image generation, they often struggle to capture fine-grained details from lengthy prompts and maintain contextual coherence within prompt sequences. Moreover, they often result in misaligned image generation for prompt sequences featuring multiple objects. To address this, we propose a Multi-modal Generation via Cross-Modal In-Context Learning (MGCC) method that generates novel images from complex multimodal prompt sequences by leveraging the combined capabilities of large language models (LLMs) and diffusion models. Our MGCC comprises a novel Cross-Modal Refinement module to explicitly learn cross-modal dependencies between the text and image in the LLM embedding space, and a contextual object grounding module to generate object bounding boxes specifically targeting scenes with multiple objects. Our MGCC demonstrates a diverse range of multimodal capabilities, like novel image generation, the facilitation of multimodal dialogue, and generation of texts. Experimental evaluations on two benchmark datasets, demonstrate the effectiveness of our method. On Visual Story Generation (VIST) dataset with multimodal inputs, our MGCC achieves a CLIP Similarity score of $0.652$ compared to SOTA GILL $0.641$. Similarly, on Visual Dialogue Context (VisDial) having lengthy dialogue sequences, our MGCC achieves an impressive CLIP score of $0.660$, largely outperforming existing SOTA method scoring $0.645$. Code: https://github.com/VIROBO-15/MGCC","sentences":["In this work, we study the problem of generating novel images from complex multimodal prompt sequences.","While existing methods achieve promising results for text-to-image generation, they often struggle to capture fine-grained details from lengthy prompts and maintain contextual coherence within prompt sequences.","Moreover, they often result in misaligned image generation for prompt sequences featuring multiple objects.","To address this, we propose a Multi-modal Generation via Cross-Modal In-Context Learning (MGCC) method that generates novel images from complex multimodal prompt sequences by leveraging the combined capabilities of large language models (LLMs) and diffusion models.","Our MGCC comprises a novel Cross-Modal Refinement module to explicitly learn cross-modal dependencies between the text and image in the LLM embedding space, and a contextual object grounding module to generate object bounding boxes specifically targeting scenes with multiple objects.","Our MGCC demonstrates a diverse range of multimodal capabilities, like novel image generation, the facilitation of multimodal dialogue, and generation of texts.","Experimental evaluations on two benchmark datasets, demonstrate the effectiveness of our method.","On Visual Story Generation (VIST) dataset with multimodal inputs, our MGCC achieves a CLIP Similarity score of $0.652$ compared to SOTA GILL $0.641$. Similarly, on Visual Dialogue Context (VisDial) having lengthy dialogue sequences, our MGCC achieves an impressive CLIP score of $0.660$, largely outperforming existing SOTA method scoring $0.645$. Code: https://github.com/VIROBO-15/MGCC"],"url":"http://arxiv.org/abs/2405.18304v1"}
{"created":"2024-05-28 15:57:58","title":"Deep Network Pruning: A Comparative Study on CNNs in Face Recognition","abstract":"The widespread use of mobile devices for all kind of transactions makes necessary reliable and real-time identity authentication, leading to the adoption of face recognition (FR) via the cameras embedded in such devices. Progress of deep Convolutional Neural Networks (CNNs) has provided substantial advances in FR. Nonetheless, the size of state-of-the-art architectures is unsuitable for mobile deployment, since they often encompass hundreds of megabytes and millions of parameters. We address this by studying methods for deep network compression applied to FR. In particular, we apply network pruning based on Taylor scores, where less important filters are removed iteratively. The method is tested on three networks based on the small SqueezeNet (1.24M parameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M) architectures. These have been selected to showcase the method on CNNs with different complexities and sizes. We observe that a substantial percentage of filters can be removed with minimal performance loss. Also, filters with the highest amount of output channels tend to be removed first, suggesting that high-dimensional spaces within popular CNNs are over-dimensionated.","sentences":["The widespread use of mobile devices for all kind of transactions makes necessary reliable and real-time identity authentication, leading to the adoption of face recognition (FR) via the cameras embedded in such devices.","Progress of deep Convolutional Neural Networks (CNNs) has provided substantial advances in FR.","Nonetheless, the size of state-of-the-art architectures is unsuitable for mobile deployment, since they often encompass hundreds of megabytes and millions of parameters.","We address this by studying methods for deep network compression applied to FR.","In particular, we apply network pruning based on Taylor scores, where less important filters are removed iteratively.","The method is tested on three networks based on the small SqueezeNet (1.24M parameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M) architectures.","These have been selected to showcase the method on CNNs with different complexities and sizes.","We observe that a substantial percentage of filters can be removed with minimal performance loss.","Also, filters with the highest amount of output channels tend to be removed first, suggesting that high-dimensional spaces within popular CNNs are over-dimensionated."],"url":"http://arxiv.org/abs/2405.18302v1"}
{"created":"2024-05-28 15:53:02","title":"CompetEvo: Towards Morphological Evolution from Competition","abstract":"Training an agent to adapt to specific tasks through co-optimization of morphology and control has widely attracted attention. However, whether there exists an optimal configuration and tactics for agents in a multiagent competition scenario is still an issue that is challenging to definitively conclude. In this context, we propose competitive evolution (CompetEvo), which co-evolves agents' designs and tactics in confrontation. We build arenas consisting of three animals and their evolved derivatives, placing agents with different morphologies in direct competition with each other. The results reveal that our method enables agents to evolve a more suitable design and strategy for fighting compared to fixed-morph agents, allowing them to obtain advantages in combat scenarios. Moreover, we demonstrate the amazing and impressive behaviors that emerge when confrontations are conducted under asymmetrical morphs.","sentences":["Training an agent to adapt to specific tasks through co-optimization of morphology and control has widely attracted attention.","However, whether there exists an optimal configuration and tactics for agents in a multiagent competition scenario is still an issue that is challenging to definitively conclude.","In this context, we propose competitive evolution (CompetEvo), which co-evolves agents' designs and tactics in confrontation.","We build arenas consisting of three animals and their evolved derivatives, placing agents with different morphologies in direct competition with each other.","The results reveal that our method enables agents to evolve a more suitable design and strategy for fighting compared to fixed-morph agents, allowing them to obtain advantages in combat scenarios.","Moreover, we demonstrate the amazing and impressive behaviors that emerge when confrontations are conducted under asymmetrical morphs."],"url":"http://arxiv.org/abs/2405.18300v1"}
{"created":"2024-05-28 15:51:18","title":"Deep Learning Innovations for Underwater Waste Detection: An In-Depth Analysis","abstract":"Addressing the issue of submerged underwater trash is crucial for safeguarding aquatic ecosystems and preserving marine life. While identifying debris present on the surface of water bodies is straightforward, assessing the underwater submerged waste is a challenge due to the image distortions caused by factors such as light refraction, absorption, suspended particles, color shifts, and occlusion. This paper conducts a comprehensive review of state-of-the-art architectures and on the existing datasets to establish a baseline for submerged waste and trash detection. The primary goal remains to establish the benchmark of the object localization techniques to be leveraged by advanced underwater sensors and autonomous underwater vehicles. The ultimate objective is to explore the underwater environment, to identify, and remove underwater debris. The absence of benchmarks (dataset or algorithm) in many researches emphasizes the need for a more robust algorithmic solution. Through this research, we aim to give performance comparative analysis of various underwater trash detection algorithms.","sentences":["Addressing the issue of submerged underwater trash is crucial for safeguarding aquatic ecosystems and preserving marine life.","While identifying debris present on the surface of water bodies is straightforward, assessing the underwater submerged waste is a challenge due to the image distortions caused by factors such as light refraction, absorption, suspended particles, color shifts, and occlusion.","This paper conducts a comprehensive review of state-of-the-art architectures and on the existing datasets to establish a baseline for submerged waste and trash detection.","The primary goal remains to establish the benchmark of the object localization techniques to be leveraged by advanced underwater sensors and autonomous underwater vehicles.","The ultimate objective is to explore the underwater environment, to identify, and remove underwater debris.","The absence of benchmarks (dataset or algorithm) in many researches emphasizes the need for a more robust algorithmic solution.","Through this research, we aim to give performance comparative analysis of various underwater trash detection algorithms."],"url":"http://arxiv.org/abs/2405.18299v1"}
{"created":"2024-05-28 15:50:10","title":"Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training","abstract":"Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations. Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension. Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training. Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA.","sentences":["Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations.","Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics.","To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model.","We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension.","Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training.","Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias.","We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA."],"url":"http://arxiv.org/abs/2405.18296v1"}
{"created":"2024-05-28 15:48:39","title":"Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention","abstract":"In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as \"I want something to support my back\". Closely related, 3D visual grounding focuses on understanding human reference. To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (\"pillow\" in this case), and finally provide a reference to the AI system, such as \"A pillow on the couch\". Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention. To tackle this challenge, we introduce the new Intent3D dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset. We also establish several baselines based on different language-based 3D object detection models on our benchmark. Finally, we propose IntentNet, our unique approach, designed to tackle this intention-based detection problem. It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization.","sentences":["In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions.","This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as \"I want something to support my back\".","Closely related, 3D visual grounding focuses on understanding human reference.","To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (\"pillow\" in this case), and finally provide a reference to the AI system, such as \"A pillow on the couch\".","Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention.","To tackle this challenge, we introduce the new Intent3D dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset.","We also establish several baselines based on different language-based 3D object detection models on our benchmark.","Finally, we propose IntentNet, our unique approach, designed to tackle this intention-based detection problem.","It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization."],"url":"http://arxiv.org/abs/2405.18295v1"}
{"created":"2024-05-28 15:48:27","title":"CF-OPT: Counterfactual Explanations for Structured Prediction","abstract":"Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.","sentences":["Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications.","Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver.","Our goal is to improve the transparency of such methods by providing counterfactual explanations.","We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations.","We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context.","These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures.","Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature."],"url":"http://arxiv.org/abs/2405.18293v1"}
{"created":"2024-05-28 15:47:11","title":"Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning","abstract":"Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications. However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of factual knowledge. In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT's limitations in knowledge learning task. Our findings reveal that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features. Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning. Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research.","sentences":["Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications.","However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of factual knowledge.","In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT's limitations in knowledge learning task.","Our findings reveal that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features.","Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning.","Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research."],"url":"http://arxiv.org/abs/2405.18292v1"}
{"created":"2024-05-28 15:43:29","title":"FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning","abstract":"Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions. Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness. However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients. This oversight inevitably decreases both fairness and model accuracy in practice. To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee. First, we present the concept of \"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring rewards to individual clients based on their contributions. Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness. This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models. Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy. Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy. We see this work as a significant step towards incentivizing broader client participation in federated learning. The source code is available at https://github.com/wangzihuixmu/FedSAC.","sentences":["Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions.","Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness.","However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients.","This oversight inevitably decreases both fairness and model accuracy in practice.","To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee.","First, we present the concept of \"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring rewards to individual clients based on their contributions.","Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness.","This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models.","Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy.","Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy.","We see this work as a significant step towards incentivizing broader client participation in federated learning.","The source code is available at https://github.com/wangzihuixmu/FedSAC."],"url":"http://arxiv.org/abs/2405.18291v1"}
{"created":"2024-05-28 15:42:45","title":"Highway Reinforcement Learning","abstract":"Learning from multi-step off-policy data collected by a set of policies is a core problem of reinforcement learning (RL). Approaches based on importance sampling (IS) often suffer from large variances due to products of IS ratios. Typical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time steps along the trajectory of actions (where $n$ is called the lookahead depth) and utilize off-policy data directly without any additional adjustment. They work well for proper choices of $n$. We show, however, that such IS-free methods underestimate the optimal value function (VF), especially for large $n$, restricting their capacity to efficiently utilize information from distant future time steps. To overcome this problem, we introduce a novel, IS-free, multi-step off-policy method that avoids the underestimation issue and converges to the optimal VF. At its core lies a simple but non-trivial \\emph{highway gate}, which controls the information flow from the distant future by comparing it to a threshold. The highway gate guarantees convergence to the optimal VF for arbitrary $n$ and arbitrary behavioral policies. It gives rise to a novel family of off-policy RL algorithms that safely learn even when $n$ is very large, facilitating rapid credit assignment from the far future to the past. On tasks with greatly delayed rewards, including video games where the reward is given only at the end of the game, our new methods outperform many existing multi-step off-policy algorithms.","sentences":["Learning from multi-step off-policy data collected by a set of policies is a core problem of reinforcement learning (RL).","Approaches based on importance sampling (IS) often suffer from large variances due to products of IS ratios.","Typical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time steps along the trajectory of actions (where $n$ is called the lookahead depth) and utilize off-policy data directly without any additional adjustment.","They work well for proper choices of $n$. We show, however, that such IS-free methods underestimate the optimal value function (VF), especially for large $n$, restricting their capacity to efficiently utilize information from distant future time steps.","To overcome this problem, we introduce a novel, IS-free, multi-step off-policy method that avoids the underestimation issue and converges to the optimal VF.","At its core lies a simple but non-trivial \\emph{highway gate}, which controls the information flow from the distant future by comparing it to a threshold.","The highway gate guarantees convergence to the optimal VF for arbitrary $n$ and arbitrary behavioral policies.","It gives rise to a novel family of off-policy RL algorithms that safely learn even when $n$ is very large, facilitating rapid credit assignment from the far future to the past.","On tasks with greatly delayed rewards, including video games where the reward is given only at the end of the game, our new methods outperform many existing multi-step off-policy algorithms."],"url":"http://arxiv.org/abs/2405.18289v1"}
{"created":"2024-05-28 15:34:33","title":"MODL: Multilearner Online Deep Learning","abstract":"Online deep learning solves the problem of learning from streams of data, reconciling two opposing objectives: learn fast and learn deep. Existing work focuses almost exclusively on exploring pure deep learning solutions, which are much better suited to handle the \"deep\" than the \"fast\" part of the online learning equation. In our work, we propose a different paradigm, based on a hybrid multilearner approach. First, we develop a fast online logistic regression learner. This learner does not rely on backpropagation. Instead, it uses closed form recursive updates of model parameters, handling the fast learning part of the online learning problem. We then analyze the existing online deep learning theory and show that the widespread ODL approach, currently operating at complexity $O(L^2)$ in terms of the number of layers $L$, can be equivalently implemented in $O(L)$ complexity. This further leads us to the cascaded multilearner design, in which multiple shallow and deep learners are co-trained to solve the online learning problem in a cooperative, synergistic fashion. We show that this approach achieves state-of-the-art results on common online learning datasets, while also being able to handle missing features gracefully. Our code is publicly available at https://github.com/AntonValk/MODL.","sentences":["Online deep learning solves the problem of learning from streams of data, reconciling two opposing objectives: learn fast and learn deep.","Existing work focuses almost exclusively on exploring pure deep learning solutions, which are much better suited to handle the \"deep\" than the \"fast\" part of the online learning equation.","In our work, we propose a different paradigm, based on a hybrid multilearner approach.","First, we develop a fast online logistic regression learner.","This learner does not rely on backpropagation.","Instead, it uses closed form recursive updates of model parameters, handling the fast learning part of the online learning problem.","We then analyze the existing online deep learning theory and show that the widespread ODL approach, currently operating at complexity $O(L^2)$ in terms of the number of layers $L$, can be equivalently implemented in $O(L)$ complexity.","This further leads us to the cascaded multilearner design, in which multiple shallow and deep learners are co-trained to solve the online learning problem in a cooperative, synergistic fashion.","We show that this approach achieves state-of-the-art results on common online learning datasets, while also being able to handle missing features gracefully.","Our code is publicly available at https://github.com/AntonValk/MODL."],"url":"http://arxiv.org/abs/2405.18281v1"}
{"created":"2024-05-28 15:25:04","title":"Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance","abstract":"Relevance and fairness are two major objectives of recommender systems (RSs). Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures). While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted. We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other? To what extent do they agree with relevance/fairness measures? How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations? We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders. We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive. We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range.","sentences":["Relevance and fairness are two major objectives of recommender systems (RSs).","Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures).","While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted.","We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other?","To what extent do they agree with relevance/fairness measures?","How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations?","We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders.","We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive.","We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range."],"url":"http://arxiv.org/abs/2405.18276v1"}
{"created":"2024-05-28 15:23:46","title":"Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach","abstract":"Since the rise of Large Language Models (LLMs) a couple of years ago, researchers in metaheuristics (MHs) have wondered how to use their power in a beneficial way within their algorithms. This paper introduces a novel approach that leverages LLMs as pattern recognition tools to improve MHs. The resulting hybrid method, tested in the context of a social network-based combinatorial optimization problem, outperforms existing state-of-the-art approaches that combine machine learning with MHs regarding the obtained solution quality. By carefully designing prompts, we demonstrate that the output obtained from LLMs can be used as problem knowledge, leading to improved results. Lastly, we acknowledge LLMs' potential drawbacks and limitations and consider it essential to examine them to advance this type of research further.","sentences":["Since the rise of Large Language Models (LLMs) a couple of years ago, researchers in metaheuristics (MHs) have wondered how to use their power in a beneficial way within their algorithms.","This paper introduces a novel approach that leverages LLMs as pattern recognition tools to improve MHs.","The resulting hybrid method, tested in the context of a social network-based combinatorial optimization problem, outperforms existing state-of-the-art approaches that combine machine learning with MHs regarding the obtained solution quality.","By carefully designing prompts, we demonstrate that the output obtained from LLMs can be used as problem knowledge, leading to improved results.","Lastly, we acknowledge LLMs' potential drawbacks and limitations and consider it essential to examine them to advance this type of research further."],"url":"http://arxiv.org/abs/2405.18272v1"}
{"created":"2024-05-28 15:16:21","title":"Error-Free and Current-Driven Synthetic Antiferromagnetic Domain Wall Memory Enabled by Channel Meandering","abstract":"We propose a new type of multi-bit and energy-efficient magnetic memory based on current-driven, field-free, and highly controlled domain wall motion. A meandering domain wall channel with precisely interspersed pinning regions provides the multi-bit capability of a magnetic tunnel junction. The magnetic free layer of the memory device has perpendicular magnetic anisotropy and interfacial Dzyaloshinskii-Moriya interaction, so that spin-orbit torques induce efficient domain wall motion. Using micromagnetic simulations, we find two pinning mechanisms that lead to different cell designs: two-way switching and four-way switching. The memory cell design choices and the physics behind these pinning mechanisms are discussed in detail. Furthermore, we show that switching reliability and speed may be significantly improved by replacing the ferromagnetic free layer with a synthetic antiferromagnetic layer. Switching behavior and material choices will be discussed for the two implementations.","sentences":["We propose a new type of multi-bit and energy-efficient magnetic memory based on current-driven, field-free, and highly controlled domain wall motion.","A meandering domain wall channel with precisely interspersed pinning regions provides the multi-bit capability of a magnetic tunnel junction.","The magnetic free layer of the memory device has perpendicular magnetic anisotropy and interfacial Dzyaloshinskii-Moriya interaction, so that spin-orbit torques induce efficient domain wall motion.","Using micromagnetic simulations, we find two pinning mechanisms that lead to different cell designs: two-way switching and four-way switching.","The memory cell design choices and the physics behind these pinning mechanisms are discussed in detail.","Furthermore, we show that switching reliability and speed may be significantly improved by replacing the ferromagnetic free layer with a synthetic antiferromagnetic layer.","Switching behavior and material choices will be discussed for the two implementations."],"url":"http://arxiv.org/abs/2405.18261v1"}
{"created":"2024-05-28 15:13:29","title":"A Vlogger-augmented Graph Neural Network Model for Micro-video Recommendation","abstract":"Existing micro-video recommendation models exploit the interactions between users and micro-videos and/or multi-modal information of micro-videos to predict the next micro-video a user will watch, ignoring the information related to vloggers, i.e., the producers of micro-videos. However, in micro-video scenarios, vloggers play a significant role in user-video interactions, since vloggers generally focus on specific topics and users tend to follow the vloggers they are interested in. Therefore, in the paper, we propose a vlogger-augmented graph neural network model VA-GNN, which takes the effect of vloggers into consideration. Specifically, we construct a tripartite graph with users, micro-videos, and vloggers as nodes, capturing user preferences from different views, i.e., the video-view and the vlogger-view. Moreover, we conduct cross-view contrastive learning to keep the consistency between node embeddings from the two different views. Besides, when predicting the next user-video interaction, we adaptively combine the user preferences for a video itself and its vlogger. We conduct extensive experiments on two real-world datasets. The experimental results show that VA-GNN outperforms multiple existing GNN-based recommendation models.","sentences":["Existing micro-video recommendation models exploit the interactions between users and micro-videos and/or multi-modal information of micro-videos to predict the next micro-video a user will watch, ignoring the information related to vloggers, i.e., the producers of micro-videos.","However, in micro-video scenarios, vloggers play a significant role in user-video interactions, since vloggers generally focus on specific topics and users tend to follow the vloggers they are interested in.","Therefore, in the paper, we propose a vlogger-augmented graph neural network model VA-GNN, which takes the effect of vloggers into consideration.","Specifically, we construct a tripartite graph with users, micro-videos, and vloggers as nodes, capturing user preferences from different views, i.e., the video-view and the vlogger-view.","Moreover, we conduct cross-view contrastive learning to keep the consistency between node embeddings from the two different views.","Besides, when predicting the next user-video interaction, we adaptively combine the user preferences for a video itself and its vlogger.","We conduct extensive experiments on two real-world datasets.","The experimental results show that VA-GNN outperforms multiple existing GNN-based recommendation models."],"url":"http://arxiv.org/abs/2405.18260v1"}
{"created":"2024-05-28 15:13:19","title":"Ranking with Ties based on Noisy Performance Data","abstract":"We consider the problem of ranking a set of objects based on their performance when the measurement of said performance is subject to noise. In this scenario, the performance is measured repeatedly, resulting in a range of measurements for each object. If the ranges of two objects do not overlap, then we consider one object as 'better' than the other, and we expect it to receive a higher rank; if, however, the ranges overlap, then the objects are incomparable, and we wish them to be assigned the same rank. Unfortunately, the incomparability relation of ranges is in general not transitive; as a consequence, in general the two requirements cannot be satisfied simultaneously, i.e., it is not possible to guarantee both distinct ranks for objects with separated ranges, and same rank for objects with overlapping ranges. This conflict leads to more than one reasonable way to rank a set of objects. In this paper, we explore the ambiguities that arise when ranking with ties, and define a set of reasonable rankings, which we call partial rankings. We develop and analyse three different methodologies to compute a partial ranking. Finally, we show how performance differences among objects can be investigated with the help of partial ranking.","sentences":["We consider the problem of ranking a set of objects based on their performance when the measurement of said performance is subject to noise.","In this scenario, the performance is measured repeatedly, resulting in a range of measurements for each object.","If the ranges of two objects do not overlap, then we consider one object as 'better' than the other, and we expect it to receive a higher rank; if, however, the ranges overlap, then the objects are incomparable, and we wish them to be assigned the same rank.","Unfortunately, the incomparability relation of ranges is in general not transitive; as a consequence, in general the two requirements cannot be satisfied simultaneously, i.e., it is not possible to guarantee both distinct ranks for objects with separated ranges, and same rank for objects with overlapping ranges.","This conflict leads to more than one reasonable way to rank a set of objects.","In this paper, we explore the ambiguities that arise when ranking with ties, and define a set of reasonable rankings, which we call partial rankings.","We develop and analyse three different methodologies to compute a partial ranking.","Finally, we show how performance differences among objects can be investigated with the help of partial ranking."],"url":"http://arxiv.org/abs/2405.18259v1"}
{"created":"2024-05-28 15:11:17","title":"Text-only Synthesis for Image Captioning","abstract":"From paired image-text training to text-only training for image captioning, the pursuit of relaxing the requirements for high-cost and large-scale annotation of good quality data remains consistent. In this paper, we propose Text-only Synthesis for Image Captioning (ToCa), which further advances this relaxation with fewer human labor and less computing time. Specifically, we deconstruct caption text into structures and lexical words, which serve as the fundamental components of the caption. By combining different structures and lexical words as inputs to the large language model, massive captions that contain various patterns of lexical words are generated. This method not only approaches the target domain but also surpasses it by generating new captions, thereby enhancing the zero-shot generalization ability of the model. Considering the different levels of data access in the real world, we define three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and data-efficient synthesis. Experiments in these scenarios demonstrate the generalizability, transferability and practicability of ToCa with a nearly 5 CIDEr improvement for zero-shot cross-domain captioning and a maximum increase of over 20 CIDEr for data-efficient captioning.","sentences":["From paired image-text training to text-only training for image captioning, the pursuit of relaxing the requirements for high-cost and large-scale annotation of good quality data remains consistent.","In this paper, we propose Text-only Synthesis for Image Captioning (ToCa), which further advances this relaxation with fewer human labor and less computing time.","Specifically, we deconstruct caption text into structures and lexical words, which serve as the fundamental components of the caption.","By combining different structures and lexical words as inputs to the large language model, massive captions that contain various patterns of lexical words are generated.","This method not only approaches the target domain but also surpasses it by generating new captions, thereby enhancing the zero-shot generalization ability of the model.","Considering the different levels of data access in the real world, we define three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and data-efficient synthesis.","Experiments in these scenarios demonstrate the generalizability, transferability and practicability of ToCa with a nearly 5 CIDEr improvement for zero-shot cross-domain captioning and a maximum increase of over 20 CIDEr for data-efficient captioning."],"url":"http://arxiv.org/abs/2405.18258v1"}
{"created":"2024-05-28 15:07:50","title":"Channel Reciprocity Based Attack Detection for Securing UWB Ranging by Autoencoder","abstract":"A variety of ranging threats represented by Ghost Peak attack have raised concerns regarding the security performance of Ultra-Wide Band (UWB) systems with the finalization of the IEEE 802.15.4z standard. Based on channel reciprocity, this paper proposes a low complexity attack detection scheme that compares Channel Impulse Response (CIR) features of both ranging sides utilizing an autoencoder with the capability of data compression and feature extraction. Taking Ghost Peak attack as an example, this paper demonstrates the effectiveness, feasibility and generalizability of the proposed attack detection scheme through simulation and experimental validation. The proposed scheme achieves an attack detection success rate of over 99% and can be implemented in current systems at low cost.","sentences":["A variety of ranging threats represented by Ghost Peak attack have raised concerns regarding the security performance of Ultra-Wide Band (UWB) systems with the finalization of the IEEE 802.15.4z standard.","Based on channel reciprocity, this paper proposes a low complexity attack detection scheme that compares Channel Impulse Response (CIR) features of both ranging sides utilizing an autoencoder with the capability of data compression and feature extraction.","Taking Ghost Peak attack as an example, this paper demonstrates the effectiveness, feasibility and generalizability of the proposed attack detection scheme through simulation and experimental validation.","The proposed scheme achieves an attack detection success rate of over 99% and can be implemented in current systems at low cost."],"url":"http://arxiv.org/abs/2405.18255v1"}
{"created":"2024-05-28 15:04:17","title":"Truthful Dataset Valuation by Pointwise Mutual Information","abstract":"A common way to evaluate a dataset in ML involves training a model on this dataset and assessing the model's performance on a test set. However, this approach has two issues: (1) it may incentivize undesirable data manipulation in data marketplaces, as the self-interested data providers seek to modify the dataset to maximize their evaluation scores; (2) it may select datasets that overfit to potentially small test sets. We propose a new data valuation method that provably guarantees the following: data providers always maximize their expected score by truthfully reporting their observed data. Any manipulation of the data, including but not limited to data duplication, adding random data, data removal, or re-weighting data from different groups, cannot increase their expected score. Our method, following the paradigm of proper scoring rules, measures the pointwise mutual information (PMI) of the test dataset and the evaluated dataset. However, computing the PMI of two datasets is challenging. We introduce a novel PMI measuring method that greatly improves tractability within Bayesian machine learning contexts. This is accomplished through a new characterization of PMI that relies solely on the posterior probabilities of the model parameter at an arbitrarily selected value. Finally, we support our theoretical results with simulations and further test the effectiveness of our data valuation method in identifying the top datasets among multiple data providers. Interestingly, our method outperforms the standard approach of selecting datasets based on the trained model's test performance, suggesting that our truthful valuation score can also be more robust to overfitting.","sentences":["A common way to evaluate a dataset in ML involves training a model on this dataset and assessing the model's performance on a test set.","However, this approach has two issues: (1) it may incentivize undesirable data manipulation in data marketplaces, as the self-interested data providers seek to modify the dataset to maximize their evaluation scores; (2) it may select datasets that overfit to potentially small test sets.","We propose a new data valuation method that provably guarantees the following: data providers always maximize their expected score by truthfully reporting their observed data.","Any manipulation of the data, including but not limited to data duplication, adding random data, data removal, or re-weighting data from different groups, cannot increase their expected score.","Our method, following the paradigm of proper scoring rules, measures the pointwise mutual information (PMI) of the test dataset and the evaluated dataset.","However, computing the PMI of two datasets is challenging.","We introduce a novel PMI measuring method that greatly improves tractability within Bayesian machine learning contexts.","This is accomplished through a new characterization of PMI that relies solely on the posterior probabilities of the model parameter at an arbitrarily selected value.","Finally, we support our theoretical results with simulations and further test the effectiveness of our data valuation method in identifying the top datasets among multiple data providers.","Interestingly, our method outperforms the standard approach of selecting datasets based on the trained model's test performance, suggesting that our truthful valuation score can also be more robust to overfitting."],"url":"http://arxiv.org/abs/2405.18253v1"}
{"created":"2024-05-28 15:02:09","title":"Sensor-Based Distributionally Robust Control for Safe Robot Navigation in Dynamic Environments","abstract":"We introduce a novel method for safe mobile robot navigation in dynamic, unknown environments, utilizing onboard sensing to impose safety constraints without the need for accurate map reconstruction. Traditional methods typically rely on detailed map information to synthesize safe stabilizing controls for mobile robots, which can be computationally demanding and less effective, particularly in dynamic operational conditions. By leveraging recent advances in distributionally robust optimization, we develop a distributionally robust control barrier function (DR-CBF) constraint that directly processes range sensor data to impose safety constraints. Coupling this with a control Lyapunov function (CLF) for path tracking, we demonstrate that our CLF-DR-CBF control synthesis method achieves safe, efficient, and robust navigation in uncertain dynamic environments. We demonstrate the effectiveness of our approach in simulated and real autonomous robot navigation experiments, marking a substantial advancement in real-time safety guarantees for mobile robots.","sentences":["We introduce a novel method for safe mobile robot navigation in dynamic, unknown environments, utilizing onboard sensing to impose safety constraints without the need for accurate map reconstruction.","Traditional methods typically rely on detailed map information to synthesize safe stabilizing controls for mobile robots, which can be computationally demanding and less effective, particularly in dynamic operational conditions.","By leveraging recent advances in distributionally robust optimization, we develop a distributionally robust control barrier function (DR-CBF) constraint that directly processes range sensor data to impose safety constraints.","Coupling this with a control Lyapunov function (CLF) for path tracking, we demonstrate that our CLF-DR-CBF control synthesis method achieves safe, efficient, and robust navigation in uncertain dynamic environments.","We demonstrate the effectiveness of our approach in simulated and real autonomous robot navigation experiments, marking a substantial advancement in real-time safety guarantees for mobile robots."],"url":"http://arxiv.org/abs/2405.18251v1"}
{"created":"2024-05-28 14:58:43","title":"Extreme Value Monte Carlo Tree Search","abstract":"Despite being successful in board games and reinforcement learning (RL), UCT, a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB), has had limited success in domain-independent planning until recently. Previous work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate for estimating the distance-to-go which are potentially unbounded in $\\mathbb{R}$, such as heuristic functions used in classical planning, then proposed combining MCTS with MABs designed for Gaussian reward distributions and successfully improved the performance. In this paper, we further sharpen our understanding of ideal bandits for planning tasks. Existing work has two issues: First, while Gaussian MABs no longer over-specify the distances as $h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are non-negative and can be further bounded in some cases. Second, there is no theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014) that backpropagates minimum/maximum of samples. We identified \\emph{extreme value} statistics as a theoretical framework that resolves both issues at once and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for classical planning. We formally prove their regret bounds and empirically demonstrate their performance in classical planning.","sentences":["Despite being successful in board games and reinforcement learning (RL), UCT, a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB), has had limited success in domain-independent planning until recently.","Previous work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate for estimating the distance-to-go which are potentially unbounded in $\\mathbb{R}$, such as heuristic functions used in classical planning, then proposed combining MCTS with MABs designed for Gaussian reward distributions and successfully improved the performance.","In this paper, we further sharpen our understanding of ideal bandits for planning tasks.","Existing work has two issues: First, while Gaussian MABs no longer over-specify the distances as $h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are non-negative and can be further bounded in some cases.","Second, there is no theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014) that backpropagates minimum/maximum of samples.","We identified \\emph{extreme value} statistics as a theoretical framework that resolves both issues at once and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for classical planning.","We formally prove their regret bounds and empirically demonstrate their performance in classical planning."],"url":"http://arxiv.org/abs/2405.18248v1"}
{"created":"2024-05-28 14:58:07","title":"Utilitarian Algorithm Configuration for Infinite Parameter Spaces","abstract":"Utilitarian algorithm configuration is a general-purpose technique for automatically searching the parameter space of a given algorithm to optimize its performance, as measured by a given utility function, on a given set of inputs. Recently introduced utilitarian configuration procedures offer optimality guarantees about the returned parameterization while provably adapting to the hardness of the underlying problem. However, the applicability of these approaches is severely limited by the fact that they only search a finite, relatively small set of parameters. They cannot effectively search the configuration space of algorithms with continuous or uncountable parameters. In this paper we introduce a new procedure, which we dub COUP (Continuous, Optimistic Utilitarian Procrastination). COUP is designed to search infinite parameter spaces efficiently to find good configurations quickly. Furthermore, COUP maintains the theoretical benefits of previous utilitarian configuration procedures when applied to finite parameter spaces but is significantly faster, both provably and experimentally.","sentences":["Utilitarian algorithm configuration is a general-purpose technique for automatically searching the parameter space of a given algorithm to optimize its performance, as measured by a given utility function, on a given set of inputs.","Recently introduced utilitarian configuration procedures offer optimality guarantees about the returned parameterization while provably adapting to the hardness of the underlying problem.","However, the applicability of these approaches is severely limited by the fact that they only search a finite, relatively small set of parameters.","They cannot effectively search the configuration space of algorithms with continuous or uncountable parameters.","In this paper we introduce a new procedure, which we dub COUP (Continuous, Optimistic Utilitarian Procrastination).","COUP is designed to search infinite parameter spaces efficiently to find good configurations quickly.","Furthermore, COUP maintains the theoretical benefits of previous utilitarian configuration procedures when applied to finite parameter spaces but is significantly faster, both provably and experimentally."],"url":"http://arxiv.org/abs/2405.18246v1"}
{"created":"2024-05-28 14:51:32","title":"Compiling with Arrays","abstract":"Linear algebra computations are foundational for neural networks and machine learning, often handled through arrays. While many functional programming languages feature lists and recursion, arrays in linear algebra demand constant-time access and bulk operations. To bridge this gap, some languages represent arrays as (eager) functions instead of lists. In this paper, we connect this idea to a formal logical foundation by interpreting functions as the usual negative types from polarized type theory, and arrays as the corresponding dual positive version of the function type. Positive types are defined to have a single elimination form whose computational interpretation is pattern matching. Just like (positive) product types bind two variables during pattern matching, (positive) array types bind variables with multiplicity during pattern matching. We follow a similar approach for Booleans by introducing conditionally-defined variables.   The positive formulation for the array type enables us to combine typed partial evaluation and common subexpression elimination into an elegant algorithm whose result enjoys a property we call maximal fission, which we argue can be beneficial for further optimizations. For this purpose, we present the novel intermediate representation indexed administrative normal form (AiNF), which relies on the formal logical foundation of the positive formulation for the array type to facilitate maximal loop fission and subsequent optimizations. AiNF is normal with regard to commuting conversion for both let-bindings and for-loops, leading to flat and maximally fissioned terms. We mechanize the translation and normalization from a simple surface language to AiNF, establishing that the process terminates, preserves types, and produces maximally fissioned terms.","sentences":["Linear algebra computations are foundational for neural networks and machine learning, often handled through arrays.","While many functional programming languages feature lists and recursion, arrays in linear algebra demand constant-time access and bulk operations.","To bridge this gap, some languages represent arrays as (eager) functions instead of lists.","In this paper, we connect this idea to a formal logical foundation by interpreting functions as the usual negative types from polarized type theory, and arrays as the corresponding dual positive version of the function type.","Positive types are defined to have a single elimination form whose computational interpretation is pattern matching.","Just like (positive) product types bind two variables during pattern matching, (positive) array types bind variables with multiplicity during pattern matching.","We follow a similar approach for Booleans by introducing conditionally-defined variables.   ","The positive formulation for the array type enables us to combine typed partial evaluation and common subexpression elimination into an elegant algorithm whose result enjoys a property we call maximal fission, which we argue can be beneficial for further optimizations.","For this purpose, we present the novel intermediate representation indexed administrative normal form (AiNF), which relies on the formal logical foundation of the positive formulation for the array type to facilitate maximal loop fission and subsequent optimizations.","AiNF is normal with regard to commuting conversion for both let-bindings and for-loops, leading to flat and maximally fissioned terms.","We mechanize the translation and normalization from a simple surface language to AiNF, establishing that the process terminates, preserves types, and produces maximally fissioned terms."],"url":"http://arxiv.org/abs/2405.18242v1"}
{"created":"2024-05-28 14:50:22","title":"Active Use of Latent Constituency Representation in both Humans and Large Language Models","abstract":"Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science. Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents. In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained. Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence. Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string. In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property. Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs. These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs.","sentences":["Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science.","Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents.","In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained.","Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence.","Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string.","In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property.","Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs.","These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs."],"url":"http://arxiv.org/abs/2405.18241v1"}
{"created":"2024-05-28 14:50:12","title":"MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution","abstract":"Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions. Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference. However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution. Modifying the preset resolution of a model may severely degrade the performance. In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding. The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models. Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods.","sentences":["Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions.","Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference.","However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution.","Modifying the preset resolution of a model may severely degrade the performance.","In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding.","The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image.","Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models.","Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods."],"url":"http://arxiv.org/abs/2405.18240v1"}
{"created":"2024-05-28 14:48:19","title":"The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings","abstract":"Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle. Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors. CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees' thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files. Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company. Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users' agency, trust, and possible disruption to traditional meeting norms. We discuss these concerns and their design implications for the development of GenAI meeting technology.","sentences":["Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle.","Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors.","CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees' thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files.","Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company.","Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users' agency, trust, and possible disruption to traditional meeting norms.","We discuss these concerns and their design implications for the development of GenAI meeting technology."],"url":"http://arxiv.org/abs/2405.18239v1"}
{"created":"2024-05-28 14:46:20","title":"Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression","abstract":"We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR). The fundamental goal of MLR is to learn the regression models from unlabeled observations. The EM algorithm finds extensive applications in solving the mixture of linear regressions. Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed. However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood. In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes. Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid. Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level. Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression.","sentences":["We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR).","The fundamental goal of MLR is to learn the regression models from unlabeled observations.","The EM algorithm finds extensive applications in solving the mixture of linear regressions.","Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed.","However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood.","In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes.","Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid.","Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level.","Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression."],"url":"http://arxiv.org/abs/2405.18237v1"}
{"created":"2024-05-28 14:46:03","title":"Position Paper: Think Globally, React Locally -- Bringing Real-time Reference-based Website Phishing Detection on macOS","abstract":"Background. The recent surge in phishing attacks keeps undermining the effectiveness of the traditional anti-phishing blacklist approaches. On-device anti-phishing solutions are gaining popularity as they offer faster phishing detection locally. Aim. We aim to eliminate the delay in recognizing and recording phishing campaigns in databases via on-device solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawler's scan to finish. Additionally, utilizing operating system-specific resources and frameworks, we aim to minimize the impact on system performance and depend on local processing to protect user privacy. Method. We propose a phishing detection solution that uses a combination of computer vision and on-device machine learning models to analyze websites in real time. Our reference-based approach analyzes the visual content of webpages, identifying phishing attempts through layout analysis, credential input areas detection, and brand impersonation criteria combination. Results. Our case study shows it's feasible to perform background processing on-device continuously, for the case of the web browser requiring the resource use of 16% of a single CPU core and less than 84MB of RAM on Apple M1 while maintaining the accuracy of brand logo detection at 46.6% (comparable with baselines), and of Credential Requiring Page detection at 98.1% (improving the baseline by 3.1%), within the test dataset. Conclusions. Our results demonstrate the potential of on-device, real-time phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest, e.g., email clients and messenger windows.","sentences":["Background.","The recent surge in phishing attacks keeps undermining the effectiveness of the traditional anti-phishing blacklist approaches.","On-device anti-phishing solutions are gaining popularity as they offer faster phishing detection locally.","Aim.","We aim to eliminate the delay in recognizing and recording phishing campaigns in databases via on-device solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawler's scan to finish.","Additionally, utilizing operating system-specific resources and frameworks, we aim to minimize the impact on system performance and depend on local processing to protect user privacy.","Method.","We propose a phishing detection solution that uses a combination of computer vision and on-device machine learning models to analyze websites in real time.","Our reference-based approach analyzes the visual content of webpages, identifying phishing attempts through layout analysis, credential input areas detection, and brand impersonation criteria combination.","Results.","Our case study shows it's feasible to perform background processing on-device continuously, for the case of the web browser requiring the resource use of 16% of a single CPU core and less than 84MB of RAM on Apple M1 while maintaining the accuracy of brand logo detection at 46.6% (comparable with baselines), and of Credential Requiring Page detection at 98.1% (improving the baseline by 3.1%), within the test dataset.","Conclusions.","Our results demonstrate the potential of on-device, real-time phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest, e.g., email clients and messenger windows."],"url":"http://arxiv.org/abs/2405.18236v1"}
{"created":"2024-05-28 14:45:00","title":"Cooperative Relative Localization in MAV Swarms with Ultra-wideband Ranging","abstract":"Relative localization (RL) is essential for the successful operation of micro air vehicle (MAV) swarms. Achieving accurate 3-D RL in infrastructure-free and GPS-denied environments with only distance information is a challenging problem that has not been satisfactorily solved. In this work, based on the range-based peer-to-peer RL using the ultra-wideband (UWB) ranging technique, we develop a novel UWB-based cooperative relative localization (CRL) solution that integrates the relative motion dynamics of each host-neighbor pair to build a unified dynamic model and takes the distances between the neighbors as \\textit{bonus information}. Observability analysis using differential geometry shows that the proposed CRL scheme can expand the observable subspace compared to other alternatives using only direct distances between the host agent and its neighbors. In addition, we apply the kernel-induced extended Kalman filter (EKF) to the CRL state estimation problem with the novel-designed Logarithmic-Versoria (LV) kernel to tackle heavy-tailed UWB noise. Sufficient conditions for the convergence of the fixed-point iteration involved in the estimation algorithm are also derived. Comparative Monte Carlo simulations demonstrate that the proposed CRL scheme combined with the LV-kernel EKF significantly improves the estimation accuracy owing to its robustness against both measurement outliers and incorrect measurement covariance matrix initialization. Moreover, with the LV kernel, the estimation is still satisfactory when performing the fixed-point iteration only once for reduced computational complexity.","sentences":["Relative localization (RL) is essential for the successful operation of micro air vehicle (MAV) swarms.","Achieving accurate 3-D RL in infrastructure-free and GPS-denied environments with only distance information is a challenging problem that has not been satisfactorily solved.","In this work, based on the range-based peer-to-peer RL using the ultra-wideband (UWB) ranging technique, we develop a novel UWB-based cooperative relative localization (CRL) solution that integrates the relative motion dynamics of each host-neighbor pair to build a unified dynamic model and takes the distances between the neighbors as \\textit{bonus information}.","Observability analysis using differential geometry shows that the proposed CRL scheme can expand the observable subspace compared to other alternatives using only direct distances between the host agent and its neighbors.","In addition, we apply the kernel-induced extended Kalman filter (EKF) to the CRL state estimation problem with the novel-designed Logarithmic-Versoria (LV) kernel to tackle heavy-tailed UWB noise.","Sufficient conditions for the convergence of the fixed-point iteration involved in the estimation algorithm are also derived.","Comparative Monte Carlo simulations demonstrate that the proposed CRL scheme combined with the LV-kernel EKF significantly improves the estimation accuracy owing to its robustness against both measurement outliers and incorrect measurement covariance matrix initialization.","Moreover, with the LV kernel, the estimation is still satisfactory when performing the fixed-point iteration only once for reduced computational complexity."],"url":"http://arxiv.org/abs/2405.18234v1"}
