{"text":"This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.","meta":{"url":"https://arxiv.org/abs/2402.12617"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function.","meta":{"url":"https://arxiv.org/abs/2003.01690"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator.","meta":{"url":"https://arxiv.org/abs/2003.01690"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator.","cats":{"new-technique":1,"new-tool":0}}
{"text":"We systematically analyze these attacks, and show their effectiveness for a variety of approaches like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as well as different data domains including images and audio.","meta":{"url":"https://arxiv.org/abs/2108.01644"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"More crucially, we demonstrate that even fine-tuning on another benign dataset cannot remove the backdoor hidden in the object detection model.","meta":{"url":"https://arxiv.org/abs/2205.14497"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"To extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model.","meta":{"url":"https://arxiv.org/abs/1909.01838"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model.","meta":{"url":"https://dl.acm.org/doi/10.1145/2810103.2813677"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal.","meta":{"url":"https://arxiv.org/abs/2007.14321"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"These results are obtained by exploiting and combining techniques from dynamical systems, number theory, and automata theory.","meta":{"url":"http://arxiv.org/abs/2405.07953v1"},"cats":{"new-technique":0,"new-tool":0}}
{"text":"We refine and extend Ziv's model and results regarding perfectly secure encryption of individual sequences.","meta":{"url":"http://arxiv.org/abs/2405.05752v1"},"cats":{"new-technique":0,"new-tool":0}}
{"text":"However, this method can only resist weak noise attacks.","meta":{"url":"file:///C:/Users/johnathani/git/arxiv-frontpage/index.html"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics.","meta":{"url":"http://arxiv.org/abs/2405.03516v1"},"cats":{"new-technique":1,"new-tool":0}}
{"text":"Our comprehensive experiments demonstrate that DMAVFL significantly outperforms existing attacks, and successfully circumvents SOTA defenses for malicious attacks.","meta":{"url":"http://arxiv.org/abs/2404.19582v1"},"cats":{"new-technique":1,"new-tool":0}}